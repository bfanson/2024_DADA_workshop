[
  {
    "path": "sessions/day4-shiny-apps/",
    "title": "Day 4: Shiny apps",
    "description": "Gentle introduction",
    "author": [
      {
        "name": "Ben Fanson",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nBefore starting\r\nDay‚Äôs learning objectives\r\nKey packages\r\nProject folder for today\r\nDownload\r\nQuick reminder of the week\r\n\r\nPep talk\r\nUse of AI\r\nLet‚Äôs build one together\r\nGo through a simple example\r\nWalk through the key bits\r\nSetting up the app\r\n\r\nStructure of the file\r\nUI (= user interface)\r\nStep 1: pick a layout for UI\r\nStep 2: create the inputs objects: buttons, inputs, drop-down menu\r\n\r\ninput\r\nserver\r\noutput\r\n\r\nA few tips on debugging\r\nLittle more on the server side\r\nReactivity\r\n\r\nLet‚Äôs try an analogy\r\nDeploy your App\r\nARI shiny account\r\nSetting up a shiny account\r\n\r\nlinking your Rstudio to shiny server\r\nDeploying your app\r\nArchiving/deleting your app\r\nPublishing Public apps\r\nPublic/private settings\r\n\r\nShiny Extensions\r\nUseful interactive packages for shiny\r\nleaflet\r\nmapview package\r\nleaflet package\r\n\r\nplotly\r\nggiraph\r\n\r\nLet‚Äôs get our hands dirty‚Ä¶\r\nAdditional resources\r\n\r\nA REMINDER TO HARRIET TO REMIND US TO START RECORDING\r\nBefore starting\r\nMake sure you have the shiny package installed on your computer. You can use code below or the ‚ÄúPackages‚Äù tab in lower right panel in Rstudio.\r\n\r\n\r\n  install.packages('shiny')  \r\n\r\n\r\nDay‚Äôs learning objectives\r\nHave a good understanding of what Shiny apps can do and how it might be useful for you\r\nUnderstand the core structure of Shiny apps: UI, Server, Input/Output, Reactivity\r\nLearn how to create and run an App locally as well as deploy on ARI‚Äôs shiny account\r\nFeel confident enough to grab example code and add to your app ü§û\r\nKey packages\r\n\r\nOf course there is cheatsheet to check out‚Ä¶\r\n\r\n\r\n\r\nknitr::include_graphics(\"shiny.pdf\")\r\n\r\n\r\n\r\nProject folder for today\r\nDownload\r\nThe Rproject contains everything that we have done, including today.\r\n\r\n\r\n Download Rstudio project\r\n\r\n\r\nFor background information on the dataset, see dataset summary\r\nQuick reminder of the week\r\nAs we have progressed throughout the week, we have been building up our Rproject to include all the parts of the workflow we introduced on Day 1.\r\nIn Day 1, we created our 1) Rproject, 2) folder structure, 3) stored our data, 4) created importData.R script, and 5) saved Analysis Ready DataSets (ARDS)\r\n\r\nIn Day 2, we 1) used the ards files to visualise our data, 2) ran statistical models using R scripts, 3) checked models, and then outputted predictions and parameters estimates.\r\n\r\nSsshhh‚Ä¶I have done a little renaming of files. DON‚ÄôT TELL PAUL\r\n\r\nIn Day 3, we used those result files to produce a Quarto report document.\r\n\r\nToday, we take the ‚Äúlast‚Äù step (for our workshop) and create a shiny App to provide interactive results.\r\n\r\nPep talk\r\nWell, I was going to go into basketball coach mode and give a pep talk about shiny looking intimidating, baby steps, etc.. But, I figured if you have made it to this day, nothing will scare you off.\r\nThough I will still say my final note of my pep talk‚Ä¶‚ÄúOnce you get beyond the foreignness of the code, we guarantee that with a little guidance, you can easily get started making your own apps in minutes. It is just about getting the basic structure and then having the confidence to do ‚Äùtrial-and-error‚Äù with borrowed code. Just be forewarned, shiny Apps can become addicting!!!‚Äù\r\nUse of AI\r\nI will be using AI a fair a bit today. I have found really useful for getting started with shiny apps and for the basics (mostly what we covering here), it can really help. As noted in other days, it is helpful to have a few terminology/concepts under your belt when working with AI for shiny app creation. I will try to demonstrate this through this session. I will just work with Copilot (and maybe Claude for comparison?). I have more info in the page on AI extra topic.\r\nLet‚Äôs build one together\r\nGo through a simple example\r\nWe will work through an example of building a simple app that displays a graph depending on the conditions selected.\r\nI am going to work with the built-in mtcars dataset for the example. I want an user to select a the number of cylinders a car has and plot the relationship between horsepower(hp) and fuel efficiency (mpg).\r\n\r\nThe use of built-in dataset makes it easy to replicate what we do here and play around\r\n\r\n\r\nhead(mtcars)\r\n\r\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\r\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\r\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\r\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\r\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\r\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\r\n\r\n\r\nlibrary(ggplot2)\r\nggplot(mtcars, aes(hp, mpg) ) + geom_point() + geom_smooth(method='lm') +\r\n  facet_wrap(~cyl) + labs(x='hp (horsepower)', y='mpg (mile/gallon)')\r\n\r\n\r\n\r\nIn Copilot, I typed the following:\r\n\r\nbuild me a shiny app using the mtcars dataset. The UI will have a side panel that will allow the user to select the cyl type and the main panel will have two tabs. The first tab will show a plot of the relationship between hp and mpg for the cyl selected by the user. The plot will also have the linear regression line and confidence band that was created using lm(). The second tab will have the linear model outputs extracted from the model using broom::tidy()‚Äù\r\n\r\nThe coding output are shown below:\r\n\r\n\r\n# set global objects\r\nlibrary(shiny)\r\nlibrary(ggplot2)\r\nlibrary(broom)\r\n\r\n# Define UI\r\nui <- fluidPage(\r\n  titlePanel(\"MTCars Analysis\"),\r\n  \r\n  sidebarLayout(\r\n    sidebarPanel(\r\n      selectInput(\"cyl\", \"Select Cylinder Type:\", \r\n                  choices = unique(mtcars$cyl))\r\n    ),\r\n    \r\n    mainPanel(\r\n      tabsetPanel(\r\n        tabPanel(\"Plot\", plotOutput(\"hp_mpg_plot\")),\r\n        tabPanel(\"Model Summary\", tableOutput(\"model_summary\"))\r\n      )\r\n    )\r\n  )\r\n)\r\n\r\n# Define server logic\r\nserver <- function(input, output) {\r\n  \r\n  filtered_data <- reactive({\r\n    mtcars[mtcars$cyl == input$cyl, ]\r\n  })\r\n  \r\n  output$hp_mpg_plot <- renderPlot({\r\n    data <- filtered_data()\r\n    model <- lm(mpg ~ hp, data = data)\r\n    \r\n    ggplot(data, aes(x = hp, y = mpg)) +\r\n      geom_point() +\r\n      geom_smooth(method = \"lm\", se = TRUE) +\r\n      labs(title = paste(\"HP vs MPG for\", input$cyl, \"Cylinders\"),\r\n           x = \"Horsepower (hp)\", y = \"Miles per Gallon (mpg)\")\r\n  })\r\n  \r\n  output$model_summary <- renderTable({\r\n    data <- filtered_data()\r\n    model <- lm(mpg ~ hp, data = data)\r\n    tidy(model)\r\n  })\r\n}\r\n\r\n# Run the application \r\nshinyApp(ui = ui, server = server)\r\n\r\n\r\nIt does run successfully and the UI looks like the following:\r\n\r\nWalk through the key bits\r\nSetting up the app\r\n\r\nONE vs MULTIPLE files\r\nFor your app setup, you can either select a single file called app.R [like we did in the example] or you can create two files: ui.R and server.R. What this is doing is just breaking up the one file into two, otherwise basically the same. The advantage of the latter is in larger apps and for reusing code (modularise). We will stick with one file method for rest of today.\r\nStructure of the file\r\nShiny app has a straightforward structure comprising four main components: 1) global heading; 2) ui; 3) server; and 4) shinyApp.\r\nHere is the basic structure, stripping away the guts.\r\n\r\n\r\n\r\n# 1) Define extra packages and global objects (e.g. your dataset, model results, formats...)\r\nlibrary(shiny)\r\nlibrary(tidyverse)\r\n\r\n\r\n# 2) Define UI for application [how it looks to the user]\r\nui <- fluidPage(    \r\n  \r\n)\r\n\r\n# 3) Define server logic [steps taken when app starts and user clicks something]\r\nserver <- function(input, output) {\r\n}\r\n\r\n# 4) Run the application \r\nshinyApp(ui = ui, server = server)\r\n\r\n\r\n\r\nVisual model of the file‚Ä¶\r\n\r\nLet‚Äôs go through each component‚Ä¶\r\nUI (= user interface)\r\nThis is what the user will be using to explore your data/results/visualizations.\r\nStep 1: pick a layout for UI\r\nFirst step is determining what kind of layout you want. Single panel, main panel with sidepanel, tabs. May want to check Shiny gallery or layout for ideas.\r\nIn our example, AI produced the following UI code using fluidPage() then breaking into sidebarLayout() and then again into sidebarPanel() and mainPanel(). Within the mainPanel() section, it added tabsetPanel() to get the tabs.\r\n\r\n\r\n# 2) Define UI for application [how it looks to the user]\r\nui <- fluidPage(\r\n  titlePanel(\"MTCars Analysis\"),\r\n  \r\n  sidebarLayout(\r\n    sidebarPanel(\r\n      selectInput(\"cyl\", \"Select Cylinder Type:\", \r\n                  choices = unique(mtcars$cyl))\r\n    ),\r\n    \r\n    mainPanel(\r\n      tabsetPanel(\r\n        tabPanel(\"Plot\", plotOutput(\"hp_mpg_plot\")),\r\n        tabPanel(\"Model Summary\", tableOutput(\"model_summary\"))\r\n      )\r\n    )\r\n  )\r\n)\r\n\r\n\r\nStep 2: create the inputs objects: buttons, inputs, drop-down menu\r\nOnce layout is figured out, now you add in the input objects. In our example that was a drop-down menu that allowed the user to pick cylinder size.\r\nCheck the cheatsheet for basic options for inputs or search Shiny gallery\r\n\r\nFew key takeways\r\nbasically creating html using simple functions\r\nfunctions are separated by commas!!!!\r\nAnd so many nested brackets!!!!\r\ninput\r\n\r\nIn the UI, an input list is created from the UI design and it is used to send user inputs to the server.\r\nTakeways\r\nlike an R list\r\nsetup by the UI\r\ncan only be changed by the user (immutable from being changed in the server code)\r\nserver\r\nThe server is the heart of the app. Let revisit the visual model‚Ä¶\r\n\r\nThree main key bits‚Ä¶\r\nYou have the inputs coming in that will be used\r\nR code that will take the inputs and convert to new output(s)\r\nthe output list is created and exported back to UI\r\nLet‚Äôs look a little closer at the server code\r\n\r\n\r\n# Define server logic\r\nserver <- function(input, output) {\r\n  \r\n  filtered_data <- reactive({\r\n    filter(mtcars, cyl == input$cyl)\r\n  })\r\n  \r\n  output$hp_mpg_plot <- renderPlot({\r\n    data <- filtered_data()\r\n    model <- lm(mpg ~ hp, data = data)\r\n    \r\n    ggplot(data, aes(x = hp, y = mpg)) +\r\n      geom_point() +\r\n      geom_smooth(method = \"lm\", se = TRUE) +\r\n      labs(title = paste(\"HP vs MPG for\", input$cyl, \"Cylinders\"),\r\n           x = \"Horsepower (hp)\", y = \"Miles per Gallon (mpg)\")\r\n  })\r\n  \r\n  output$model_summary <- renderTable({\r\n    data <- filtered_data()\r\n    model <- lm(mpg ~ hp, data = data)\r\n    tidy(model)\r\n  })\r\n}\r\n\r\n\r\n\r\nKey takeways\r\nnot separated by commas\r\nmore like R programming but order does not matter between components!!!!!\r\nWithin renderXXX or reactive() we have R code and order does matter here. Note the double-brackets needed when multiple lines\r\noutput\r\nOur server created output$hp_mpg_plot using renderPlot and output$model_summary using renderTable and the ui outputs using plotOutput and tableOutput. You get these pairs of renderXXX({}) and xxxOutput().\r\n\r\n\r\n# does not run...just showing R code snippets that pair up\r\n  output$hp_mpg_plot <- renderPlot({...})       # code in server side\r\n  tabPanel(\"Plot\", plotOutput(\"hp_mpg_plot\")) # code in ui side\r\n  \r\n  output$model_summary <- renderTable({...})   # code in server side\r\n  tabPanel(\"Model Summary\", tableOutput(\"model_summary\")) # code in ui side\r\n\r\n\r\n\r\nLook at the cheatsheet for a partial list of pairings‚Ä¶\r\n\r\nNote - this is not a comprehensive list and packages might have then own renderXxx() and xxxOutput()\r\nCouple more examples‚Ä¶\r\n\r\n\r\n# leaflet package (see below in the lecture)\r\n  leaflet::renderLeaflet()   # server side for leaflets\r\n  leaflet::leafletOutput()   # UI side for leaflets\r\n  \r\n# plotly package (see below in the lecture)  \r\n  renderPlotly()  # server side for plotly\r\n  plotlyOutput()  # UI side for plotly\r\n  \r\n\r\n\r\nA few tips on debugging\r\nwork in steps\r\nto test UI, you can often comment out all of the server content\r\nonce you got UI working, work on the server\r\nwith server, you can create temporary substitutes variable to test the code:\r\n\r\n\r\n#example testing parameters \r\ninput <- list(cyl=6)\r\nfiltered_data <- function() filter(mtcars, cyl==input$cyl)\r\n\r\n\r\n\r\n\r\nLittle more on the server side\r\nReactivity\r\nThe style of programming used in shiny apps is called ‚Äúreactive programming‚Äù. The purpose of this programming style is to keep inputs and outputs in sync. To do this efficiency, you only want to update outputs that need to be changed. So the rule is ‚Äúchange if and only if inputs change‚Äù.\r\nIn our example we had one such case. This reactive function was then used in both renderXXX functions.\r\n\r\n\r\n  filtered_data <- reactive({\r\n    filter(mtcars, cyl == input$cyl)\r\n  })\r\n  \r\n\r\n\r\n\r\nThe two key components of this are called:\r\nlazy - only do work when called to [‚Äúprocrastinate as long as possible‚Äù]\r\ncache - save the results produced when first called and use that until something changes\r\nYou will see a variety of functions that are doing ‚Äúextra‚Äù bits in the programming:\r\n\r\n\r\n  shiny::reactive()        # create function for repeated active (e.g. filtering dataset for multiple outputs )\r\n  shiny::reactiveValues()  # create a new reactive \"list\" like input and output\r\n  shiny::observe()         # performs a side effect, changes to input changes\r\n  shiny::observeEvent()    # performs when an event happens like submit button (e.g. input/output)\r\n  \r\n\r\n\r\nWe will not go into depth here but just want you to be aware and when you see them, know that they are doing extra stuff and a bit of efficiency stuff. For more complicated/sophisicated apps, they are essential to get a handle of.\r\nLet‚Äôs try an analogy\r\nHere is an analogy that I came across that hopefully helps. Think of the app as a restuarant‚Ä¶\r\nUI (User Interface): Think of this as the menu of the restaurant. It lists all the dishes (inputs and outputs) that the customers (users) can choose from.\r\nServer: This is the kitchen where all the cooking happens. The chefs (server functions) take the orders (inputs) from the customers and prepare the dishes (outputs). The kitchen processes the raw ingredients (data) and transforms them into delicious meals (results).\r\nReactive Expressions: These are like the recipes that the chefs follow. They define how to prepare each dish based on the ingredients and cooking methods. If an ingredient changes, the recipe ensures the dish is updated accordingly.\r\nReactive Values: These are the ingredients in the kitchen. They can be fresh produce, spices, or any other items that the chefs need to prepare the dishes. If the ingredients change, the dishes will also change.\r\nObservers: These are like the waitstaff who keep an eye on the dining room. They monitor the customers and ensure that their needs are met promptly. If a customer needs something, the waitstaff (observers) take action to address it.\r\nDeploy your App\r\nARI shiny account\r\nThanks to Nev and Jim in particular üëè, we now have a ARI shiny Professional account (called arisci). This account is available to all ARI staff though we do have limits on the number of account users (max = 25 user accounts) but it can have an unlimited number of apps and has 10,000 usage hours (per month). The shiny apps are hosted on https://www.shinyapps.io/ and have https://arisci.shinyapps.io/XXAppNameXX address (e.g.¬†https://arisci.shinyapps.io/ibisTracker/).\r\nNev/Jim have created a live document detailing the process. I will go key aspects below.\r\nSetting up a shiny account\r\nChat with others in your area and decide whether it may be useful to have a program-wide (similar) account for multiple users. If you are likely a higher user, it may be best to have your own account. If just playing around, you can get a few account on shiny.io\r\nEmail Jim Thomson [‚ÄúThe Gatekeeper‚Äù] requesting\r\nYou will receive an invite to create an account. For your account, you will use your email (or designated person) as the username and then set a password\r\nlinking your Rstudio to shiny server\r\nAll account holders can access all the apps on the shared account. To ensure that you do not mess with other user‚Äôs apps accidentally please use the rsconnect package for all you uploading, modification, archiving and deletion of apps ‚Äì your connection will have its own token. See detailed info.\r\nClick here for step-by-step. Overview is shown below:\r\n\r\nIf you prefer, you can use rsconnect package via code. Get the name, token, and secret info from the token page when logged into https://www.shinyapps.io/.\r\n\r\n\r\nrsconnect::setAccountInfo(name='arisci',\r\n                           token='XXXXX',\r\n                           secret='XXXXX' )  \r\n\r\n\r\nDeploying your app\r\nYou can use the publish approach via Rstudio (GUI approach):\r\n\r\nOr you can do the rsconnect way (just make sure the current directory is where the app is or you need to specify location)\r\n\r\n\r\n# from current directory\r\nrsconnect::deployApp( )  \r\n\r\n# or if in a different directory, specify the directory\r\n\r\nrsconnect::deployApp('app/appAmazing' )  \r\n\r\n\r\nArchiving/deleting your app\r\nTo prevent deleting of others apps, you should archive/delete the app using rsconnect code below. You can just type into Rconsole and run, assuming that you have setup your shiny connection (as shown above). You first need to archive the app and then delete:\r\n\r\n\r\nrsconnect::terminateApp(appName = 'appName')  # this will archive the app.\r\nrsconnect::purgeApp(appName='appName')        # this will delete it off the server \r\n\r\n\r\nPublishing Public apps\r\nCheck Nev/Jim live document on the current process for this.\r\nPublic/private settings\r\nYou can do via deployApp:\r\n\r\n\r\nrsconnect::deployApp(appDir = \"your app\", appVisibility = \"private\")\r\n\r\n\r\nOr you can go onto shiny.io 1) go to your app info; 2) click on users, and 3) finally public.\r\n\r\nIf private, you will need to authorise users which you can do using below:\r\n\r\n\r\n# you can add specific users\r\nrsconnect::addAuthorizedUser(email= user@emailaddress.com)\r\n\r\n\r\nIf the user does not have an account on shinyApps.io they will need to create one. They will be sent an email with an invitation link.\r\nShiny Extensions\r\nAs noted about, search the Shiny gallery to get ideas of additional options of what you can do. On this website, it provides all the ui/server code on a GitHub repository, so you can just grab the code.\r\nAnother good place to search for additional extensions for shiny is Shiny Awesome which has a curated list and some of the stuff, is well, awesome! (and overwhelming) The links often lead you to GitHub pages.\r\nUseful interactive packages for shiny\r\nAs Shiny apps are all about interactivity, you may want to use interactive plots. Check the following webpage for a examples of packages that have interactivity.\r\nI will show some interactive figures that might be of interest below:\r\nleaflet\r\nmapview package\r\nThe mapview package can be used to set up a simple, quick leaflet. This would be most useful when the leaflet is created on startup (not adding in reactivity like adding or deleting data being shown). For instance, you might want to show all the sites surveyed and attach metadata (e.g.¬†dates surveyed, number of fish caught, number of plots) to that point.\r\n\r\n\r\n# example using built-in dataset\r\n  library(mapview)\r\n  mapview::mapview(breweries)\r\n\r\n\r\n\r\nUseful shiny functions for Server/UI‚Ä¶\r\n\r\n\r\n# how it is supposed work\r\n  mapview::renderMapview()  # server side\r\n  mapview::mapviewOutput()  # UI side\r\n  \r\n# I have issues and used leaflet instead example below\r\n  sf_m <- mapview::breweries\r\n  your_map <- mapview::mapview(sf_m)\r\n  output$map <- leaflet::renderLeaflet(your_map@map)  # grabs the leaflet\r\n# then you  - server side\r\n  leaflet::leafletOutput( 'map' ) # UI side\r\n\r\n\r\nleaflet package\r\nNow, if you are going to build maps that will change with user input, it is best to build from ‚Äúscratch‚Äù using leaflet package (and probably leaflet.extra package + others)\r\nWhen you have user inputs affecting the map shown, you want to try to avoid rebuilding the map object and rather just modify the elements that the user wants changed (e.g.¬†drop lines and add points instead). This requires using reactive programming and observe() functions. Check out Nev‚Äôs ibis app that does this.\r\nUseful shiny functions for Server/UI‚Ä¶\r\n\r\n\r\n  leaflet::leafletOutput()  # UI side\r\n  leaflet::renderLeaflet()  # server side - creates the basemap\r\n  leaflet::leafletProxy()   # this is the server function that updates the map \r\n\r\n\r\n\r\nplotly\r\nNow, plotly package does a great job at interactive plots. You can write you code in ggplot and then just convert as shown in code below.\r\n\r\n\r\n  library(ggplot2)\r\n  library(plotly)\r\n  f <- ggplot( cars, aes(speed, dist)  ) + geom_point() + geom_smooth()\r\n   plotly::ggplotly(f)\r\n\r\n\r\n\r\n\r\nUseful shiny functions for Server/UI‚Ä¶\r\n\r\n\r\n  plotly::renderPlotly()\r\n  plotly::plotlyOutput()\r\n\r\n\r\nggiraph\r\nAgain, you can build your plots using ggplot and convert. I like this one for linking up plots‚Ä¶below, put your cursor over a dot on the left or a bar on the right. If you know CSS, it can be pretty powerful‚Ä¶\r\n\r\n\r\nlibrary(ggiraph)\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\n\r\nmtcars_db <- rownames_to_column(mtcars, var = \"carname\")\r\n\r\n# First plot: Scatter plot\r\nfig_pt <- ggplot(\r\n  data = mtcars_db,\r\n  mapping = aes(\r\n    x = disp, y = qsec,\r\n    tooltip = carname, data_id = carname\r\n  )\r\n) +\r\n  geom_point_interactive(\r\n    size = 3, hover_nearest = TRUE\r\n  ) +\r\n  labs(\r\n    title = \"Displacement vs Quarter Mile\",\r\n    x = \"Displacement\", y = \"Quarter Mile\"\r\n  ) +\r\n  theme_bw()\r\n\r\n# Second plot: Bar plot\r\nfig_bar <- ggplot(\r\n  data = mtcars_db,\r\n  mapping = aes(\r\n    x = reorder(carname, mpg), y = mpg,\r\n    tooltip = paste(\"Car:\", carname, \"<br>MPG:\", mpg),\r\n    data_id = carname\r\n  )\r\n) +\r\n  geom_col_interactive(fill = \"skyblue\") +\r\n  coord_flip() +\r\n  labs(\r\n    title = \"Miles per Gallon by Car\",\r\n    x = \"Car\", y = \"Miles per Gallon\"\r\n  ) +\r\n  theme_bw()\r\n\r\n# Combine the plots using patchwork\r\n combined_plot <- fig_pt + fig_bar + plot_layout(ncol = 2) \r\n\r\n# Combine the plots using cowplot\r\n# combined_plot <- cowplot::plot_grid(fig_pt, fig_bar, ncol=2) \r\n\r\n# Create a single interactive plot with both subplots\r\ninteractive_plot <- girafe(ggobj = combined_plot)\r\n\r\n# Set options for the interactive plot\r\ngirafe_options(\r\n  interactive_plot,\r\n  opts_hover(css = \"fill:cyan;stroke:black;cursor:pointer;\"),\r\n  opts_selection(type = \"single\", css = \"fill:red;stroke:black;\")\r\n)\r\n\r\n\r\n\r\n\r\nUseful shiny functions for Server/UI‚Ä¶\r\n\r\n\r\n  ggiraph::renderGirafe()\r\n  ggiraph::ggiraphOutput()\r\n  \r\n\r\n\r\nLet‚Äôs get our hands dirty‚Ä¶\r\n\r\nOption 1: Build your first app\r\ncreate a shiny app using dropdown menu in the Rproject [as we did above] -\r\ncopy the mtcars app code we used above and paste it in\r\nrun the app [highlight and run all code or click Run App]\r\nnow, have fun and play around\r\ntry changing the type on input: checkbox, radio button [see the cheatsheet]\r\nInstead of using cyl, use another column as the input\r\nchange your ggplot options\r\nadd a new tab to show the raw data\r\n\r\n\r\n\r\n\r\nOption 2: Run the appWetlandGraze in the Rproject\r\nDownload the Rproject and put in your preferred location\r\nDouble-click the 2024_WetlandGraze.Rproj to open the project\r\nOpen the appWetlandGraze/app.R file in Rstudio\r\nNotice the AI prompt that I used to get started.\r\nRun the app - Play around with the interface [brake it and then fix it]\r\n\r\n\r\n\r\nOption 3: Construct your own app from scratch\r\nuse their the 2024_WetlandGraze project or built-in dataset\r\nfigure out what you want your app to show\r\ntry to use a AI prompt to get you started or check out the shiny galleries and grab that code\r\nthink about your layout: side panel, tabs, types of input options\r\nwhat renderXxx() and xxxOutput() functions will your need?\r\n\r\nAdditional resources\r\nFor useful resources: https://shiny.posit.co/r/articles/\r\nGood for basic shiny https://mastering-shiny.org/\r\nFor high level apps, Engineering Production-Grade Shiny Apps\r\n\r\n\r\n\r\n",
    "preview": "sessions/day4-shiny-apps/distill-preview.png",
    "last_modified": "2024-09-18T22:14:35+10:00",
    "input_file": {}
  },
  {
    "path": "sessions/day3-report-writing-with-quarto/",
    "title": "Day 3: Report writing with Quarto",
    "description": "Analyse. Share. Reproduce.",
    "author": [
      {
        "name": "Paul Moloney",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nDay‚Äôs objectives\r\nKey packages\r\nProject folder for today\r\nWorkflow\r\nThe pitch\r\nWhy would I want to use something like Quarto?\r\nWhat to expect when creating a report with Quarto\r\nDon‚Äôt panic\r\nWorkflow within Quarto\r\n\r\nCreating a document using Quarto\r\nStarting a new Quarto document\r\nWorking with a pre-existing .qmd\r\nCreating tables\r\nNative to Quarto\r\nNative to R\r\n\r\nFigures\r\nInserting pictures\r\nCreating figures\r\n\r\nIncluding citations\r\nInserting footnotes\r\nDynamic text\r\nTemplates\r\nCreating your own template\r\n\r\n\r\n\r\n\r\n\r\n\r\nREMINDER TO US: START RECORDING!!!\r\n\r\n\r\nShow code\r\n\r\n# just reading in some information for this webpage\r\nds_richness <- readRDS('data/ards_richness.rds') %>% filter(!is.na(native)) %>%\r\n  mutate(visit = as.numeric(str_sub(survey_id, start = -1)) - 1)\r\ntb_mod_spp <- readRDS('results/tb_mod_spp.rds')\r\npred_spp <- readRDS('results/pred_spp.rds')\r\n\r\n\r\nDay‚Äôs objectives\r\nNot being overwhelm with using code in a markdown setting\r\nBeing able to create a document in R through Quarto\r\nBeing able to use a Word template in Quarto\r\nBeing able to construct tables in R\r\nBeing able to use citations and references inside Quarto\r\nKey packages\r\n\r\nProject folder for today\r\n\r\n\r\n Download Day 3 R project\r\n\r\n\r\nFor background information on the dataset, see dataset summary\r\nWorkflow\r\nReminder of our workflow‚Ä¶\r\n\r\nThe pitch\r\n\r\n\r\n\r\nSo, you need to analyse some data and report on the results, including tables, figures and references?\r\n\r\n\r\nDo you hate having to copy and paste the results from R, Python or SQL into Word?\r\nDo you need to produce a report or website with updated data and results on a regular basis?\r\nDo you want to produce an interactive document or dashboard?\r\nHas someone informed you that the analysis you just wrote up had some errors in the data and you need to rerun the analysis?\r\n\r\n\r\nThen maybe Quarto is for you!!!!\r\n\r\n \r\n\r\nWhy would I want to use something like Quarto?\r\nThe benefits of using something like Quarto is around closing the circle in your workflow when it comes to analysing data and producing documents with that data.\r\nQuarto allows you to seamlessly integrate work from difference like R, Python, SQL, Latex and many other sources within the one notebook interface. It has the functionality to incorporate citations and change referencing systems easily. From one notebook, you can produce a Word document or a website.\r\nThe outputs could be documents (Word, PDF, HTML) presentations (PowerPoint, Beamer, Revealjs), websites, books/manuscripts or interactive displays (Shiny, widgets).\r\nYour final output can contain formatted tables, figures, pictures, footnotes, citations, cross-references, all updated once the code is compiled. As part of your workflow, you can update reports easily, potentially with nothing more than updating the data and clicking the render button1.\r\nWhat to expect when creating a report with Quarto\r\nFor those that are used to using Word and PowerPoint to create all their work documents, using a computational document can be a bit daunting. However, our goal today is to break the ice with using a mark-up language to creating a document. With Quarto and RStudio that task is a little easier.\r\nDon‚Äôt panic\r\nThe first (and main) difference you will notice when you open a Quarto document is that there is weird coding bits here and there. Unlike Word and PowerPoint, Quarto is not WYSIWYG (What You See Is What You Get). We will use the Visual option that makes it a bit more like WYSIWYG, but not completely, as certain parts will only be converted once you render the file. To get the final version we need to compile the file.\r\n\r\nWorkflow within Quarto\r\nFor each project where you want to create a document you follow a similar pattern.\r\nOpen a file that uses the .qmd extension.\r\nWrite the content with using the Quarto syntax.\r\nEmbed R (or other) code that creates the output that you want to include in the report.\r\nRender the code, transforming the code into a slideshow, pdf, html or Word file.\r\nCreating a document using Quarto\r\nStarting a new Quarto document\r\nThe first thing that you want to do is open/create your project in RStudio. Then choose File > New file > Quarto document‚Ä¶ from the menu to create a new .qmd file. You will need to chose what sort of document you want to create. The one we will create is ‚ÄúDocument‚Äù (Figure 1). This will create a Quarto script that includes some text and examples of how to create the document that you want.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"figures/open_quarto.png\")\r\n\r\n\r\n\r\nFigure 1: Screenshot of the pop-up window you need to use to create your Quarto script.\r\n\r\n\r\n\r\nThe first thing you will notice (see Figure 2) is that unlike a Word document there is code and different coloured text, but this is not what the rendered document will look like, this is just the code to create the document. You will notice at the top of the page that there is some colourful text with names like ‚Äútitle‚Äù and ‚Äúformat‚Äù in between lines with three dashed. This is called the YAML and it sets the style and format of your document. The default YAML is written for us in the template. For the moment, change the title to something other than ‚Äúuntitled‚Äù and under that let‚Äôs add ‚Äúauthor: Your Name‚Äù with your actual name (rather than Your Name). Now press the Render button and it will render your document, with your new title and you as the author. Have a look at the html document produced and see the sort of things that can be included from R and Quarto.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"figures/initial_quarto.png\")\r\n\r\n\r\n\r\nFigure 2: Screenshot of the initial Quarto script.\r\n\r\n\r\n\r\nWorking with a pre-existing .qmd\r\nRather than working on this template let us break down the parts of the code by opening the file ‚Äúwetland grazing.qmd‚Äù. There is a lot of code, but this is basically a finished document (in terms of most coding anyway). After the YAML (which has more in it than before, which we will cover later) there is what we call a ‚Äúchunk‚Äù of code. This is where we use our R code to edit data, run models and generally do all our R stuff. The first chunk is usually where we set-up the R environment by loading libraries and set defaults for the chunks.\r\nMarkdown basics\r\nThe structure of a chunk is that inside the braces (‚Äú{ }‚Äù) you start with ‚Äúr‚Äù (if it is an R chunk). Directly under that are usually the options you want for this chunk. Common options are:\r\nCommon options for code chunks\r\nName\r\nDescription\r\necho\r\nWhether to display the code along with its results (default is TRUE)\r\ninclude\r\nWhether to display the results of the code (default is TRUE)\r\nwarning\r\nWhether to display warnings (default is TRUE)\r\ncache\r\nWhether to cache results for future renders (saves the results and uses them for later renders without re-evaluating them, default is FALSE). Can be useful if a chunk take a long to run, but can mean that if the data elsewhere is changed, it won‚Äôt be reflected in this chunk.\r\nfig-width\r\nWidth in inches for plots created in the chunk (default is 7‚Äù)\r\nfig-height\r\nHeight in inches for plots created in the chunk (default is 7‚Äù)\r\nfig-cap\r\nThe caption to be associated with this figure\r\ntab-cap\r\nThe caption to be associated with this table\r\nlabel\r\nThe name used to reference this chunk\r\n\r\n\r\n\r\n\r\nNB: If you want to be able to cross-reference your tables and figures etc, you need to follow the naming conventions, otherwise it will not be able to find the label and the cross-reference won‚Äôt show up.\r\nCreating tables\r\nTo create a table in Quarto there are two main methods.\r\nNative to Quarto\r\nYou can insert a table using the Insert > Table‚Ä¶ option through the menu. You can select the dimensions of the table, if you want headers and if you want a caption. It then constructs that table, similar to a blank table in Word. You would then need to populate that table by hand. Maybe copy and paste will work, but the whole point of using something like Quarto is to not have to copy and paste data. The ‚ÄúCommon options for code chunks‚Äù was constructed that way.\r\n‚≠ê ‚òÜ ‚òÜ ‚òÜ ‚òÜ\r\nNative to R\r\nAlternatively, you could insert an R chunk, and import a table that way from your data. If you leave it at that you will have a very basic table entered that looks like the one below.\r\n\r\n\r\nShow code\r\n\r\ndata.frame(Here = rpois(5, 3), There = rpois(5, 5))\r\n\r\n  Here There\r\n1    4     3\r\n2    2     3\r\n3    3     4\r\n4    1     4\r\n5    2     3\r\n\r\nIf you want to produce a table that is ready to publish, then you could use a package like flextable or gt to get it looking nicer, with lots of options. The table below is an example of an ARI style table using flextable. If you are wanting to reference your tables, you will need to label them with the prefix ‚Äútbl-‚Äù\r\nHere is more information on creating tables with flextable\r\n\r\n\r\nShow code\r\n\r\nimp_spp <- (tb_mod_spp$conf.low*tb_mod_spp$conf.high > 0) & (tb_mod_spp$effect == 'fixed')\r\ntb_mod_spp %>%\r\n  select(-component) %>%\r\n  mutate(across(where(is.numeric), ~ round(.x, 3)),\r\n         effect = str_to_sentence(effect),\r\n         effect = str_replace(effect, 'Ran_pars', 'Random')) %>% flextable() %>%\r\n  set_header_labels(values = c('Effect', 'Grouping', 'Parameter', 'Estimate', 'S.E.', 'Lower', 'Upper')) %>%\r\n  add_header_row(colwidths = c(5, 2), values = c(\"\", \"95% Credible bound\")) %>%\r\n  align( j=4:7, align='right', part='body' ) %>%\r\n  bold(i = imp_spp) %>%\r\n  ari_theme()\r\n\r\n95% Credible boundeffectgrouptermestimatestd.errorconf.lowconf.highFixed(Intercept)2.2050.1201.9672.442FixedgrazingPress-0.3910.196-0.780-0.023FixedfenceYes0.0050.039-0.0710.080Fixedvisit-0.0300.009-0.048-0.013FixedgrazingPress:fenceYes0.0790.070-0.0580.214FixedgrazingPress:visit-0.0160.018-0.0510.019FixedfenceYes:visit-0.0130.013-0.0370.012FixedgrazingPress:fenceYes:visit-0.0250.025-0.0730.025Randomwetlandsd__(Intercept)0.4820.0750.3620.650Randomwetland:transect_idsd__(Intercept)0.1050.0150.0760.134\r\n\r\nFigures\r\nInserting pictures\r\nYou can insert a picture by using the Insert > Figure/Image‚Ä¶ just like you would in Word. You can add a caption at that stage. To give it a label/ID select the ‚ÄúAttributes‚Äù button and type in a unique name that starts with ‚Äúfig-‚Äù so that it know its a figure.\r\n\r\nCreating figures\r\nIf you want to construct a plot (say using ggplot2) you can do that via an R chunk. Insert the R chunk from the menu (Insert > Code Cell > R) and crate your figure from scratch. Use the code chunk to set the label so you can reference it later.\r\nIncluding citations\r\nCitations can be inserted using the menu Insert > Citation‚Ä¶. That will give you a pop-up window that will link to your Zotero account and other public reference source. If you start typing in the some information about the citation you are after, it will then search your reference list or search externally if you select that option. Selected citations will appear at the bottom of the pop-up window. Once you have the citations you are after, press Insert and they will be inserted into the notebook, as well as added to the .bib file for this project.\r\nAlternatively, once you have some citations or your Zotero links, it you type ‚Äú@‚Äù it will pop-up a window to search for citations from your list.\r\nWhen you render the document, your citations (saved in the .bib file) will be sent to pandoc and any style you have selected will be applied. In the YAML you will see a line for the cls and the bibliography. Unless you want to attach a specific reference library, you shouldn‚Äôt need to change that file name. The cls on the other hand can be changed easily to change the referencing style of the document.\r\n\r\nYou can search for publication styles here\r\n\r\nInserting footnotes\r\nIt is easy to insert a footnote, just use the menu to Insert > Footnote. It will insert a number for the footnote and give a subtle pop-up for you to enter the text for that footnote. If you want to change the footnote, just click the number again.\r\nDynamic text\r\nYou can insert dynamic text into your writing in-line, by pressing ‚Äú`‚Äù (shift+tilda) followed by r then your calculation followed by another ‚Äú`‚Äù. This can be very useful when writing reports and you need to reference a statistic. If it changed, you don‚Äôt need to go back and edit it, it will update it automatically when you render it again.\r\nTemplates\r\nIf you want to customize the appearance of output into Word, Pandoc supports a special type of template called a reference document. To make Quarto use a specific reference document, it needs to be included in the YAML.\r\n\r\nCreating your own template\r\nTo create a new reference doc based on the Pandoc default, execute the following command:\r\n\r\n$ quarto pandoc -o custom-reference-doc.docx\r\n‚Äìprint-default-data-file reference.docx\r\n\r\nThen, open custom-reference-doc.docx in MS Word and modify styles as you wish. You can open the Styles pane from the HOME tab in the MS Word toolbar. When you move the cursor to a specific element in the document, an item in the styles list will be highlighted. If you want to modify the style of any type of element, you can click the drop-down menu on the highlighted item. After you finish modifying the styles, you can save the document and use it as the template for future Word documents.\r\nWord templates help\r\n\r\nWe are in no way liable for your words not matching the analysis output if the data changes‚Ü©Ô∏é\r\n",
    "preview": "sessions/day3-report-writing-with-quarto/distill-preview.png",
    "last_modified": "2024-09-18T08:40:09+10:00",
    "input_file": {}
  },
  {
    "path": "sessions/day2-analysis-workflow/",
    "title": "Day 2: Analysis workflow",
    "description": "Statistical analysis: statistical model; extracting outputs; displaying results",
    "author": [
      {
        "name": "Paul Moloney",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nDay‚Äôs objectives\r\nKey packages\r\nProject folder for today\r\nWorkflow\r\nExploring the data and developing the model\r\nMixed effects models: trying to account for correlation\r\nThe underlying distribution\r\n\r\nCreating the model\r\nAdvanced topic: Frequentist or Bayesian?\r\nCoding the model\r\nChecking model assumptions\r\nModel interpretation and displying results\r\n\r\nHands-on compenent\r\n\r\nDay‚Äôs objectives\r\nBeing able to display the raw data\r\nInterpreting how that effects the models we can use\r\nBeing able to run that statistical model using RStudio/R\r\nAssessing the model fit and assumptions\r\nInterpreting and extracting the model outputs\r\nDisplaying the model outputs to help explain the conclusions\r\nKey packages\r\n\r\nProject folder for today\r\n\r\n\r\n Download Rstudio project\r\n\r\n\r\nFor background information on the dataset, see dataset summary\r\nWorkflow\r\nReminder of our workflow‚Ä¶\r\n\r\nExploring the data and developing the model\r\n\r\n\r\nShow code\r\n\r\nds_ht <- readRDS('data/ards_height.rds') %>% filter(!is.na(wetland)) # remove NA's from data\r\nds_richness <- readRDS('data/ards_richness.rds')\r\n\r\n\r\nAfter yesterdays session we should all have a couple of data sets related to the WIMP project. One dataset (ards_height.rds) relates to the maximum plant height within a each quadrat across treatments, time, wetlands and transects. The other dataset (ards_richness.rds) relates to the native species richness within a transect across treatments, time and wetlands. The treatments were:\r\ngrazing type which has two levels:\r\nPress was generally lower intensity over a longer period\r\nCrash was generally higher intensity over a shorter period, and\r\n\r\nfence referred to whether the transect was fenced or not (Yes or No)\r\n\r\nClick over to dataset webpage for overview of the wetland data\r\nOf interest was the patterns over time, and the effect of grazing type on the wetland. Let us start by looking at a plot of the maximum plant height (max_ht) over time (dt_survey) as shown in Figure 1. The simple linear regression line added to the plot seems to indicate that there is an increase in maximum plant height over time. In R speak, that model takes the form:\r\n\\[\r\n\\text{max_ht} \\sim \\text{df_survey}\r\n\\]\r\nIn English, that could be translated to ‚Äúmaximum plant height is related to survey date‚Äù.\r\n\r\n\r\nShow code\r\n\r\nggplot(ds_ht, aes(dt_survey, max_ht)) + geom_point() +\r\n  stat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\") +\r\n  ylab('Maximum plant height (cm)') +\r\n  xlab('Time')\r\n\r\n\r\n\r\nFigure 1: Plot of maximum plant height at the quadrat over time. The blue line represent the estimate from the simple linear model\r\n\r\n\r\n\r\nThe simple linear model related to just time does not help with our understanding of the system, given within each wetland that half the transects were either fenced (excluded grazing) or unfenced (allowed grazing). We should look to see if there are any differences between those levels. In Figure 2 we can see that maybe there is a difference between slopes at fenced versus non-fenced transects. This is clear when you see that in 2018 (before fencing) the maximum plant heights are around 40cm, but by 2024 they are closer to 66cm and 55cm for fenced and unfenced respectively. This is what we call an interaction, the response to time is allowed to differ between levels of the treatment. This model would be of the form1:\r\n\r\nThese types of models are called fixed effects models, as each predictor variable is fixed or non-random quantities. The values the parameters take is important and/or the predictor variables cover the range of values of interest.\r\n\\[\r\n\\text{max_ht} \\sim \\text{df_survey} * \\text{fence}\r\n\\]\r\n\r\n\r\nShow code\r\n\r\nggplot(ds_ht, aes(dt_survey, max_ht)) + geom_point() +\r\n  stat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\") +\r\n  ylab('Maximum plant height (cm)') +\r\n  xlab('Time') +\r\n  facet_grid(. ~ fence) \r\n\r\n\r\n\r\nFigure 2: Plot of maximum plant height at the quadrat over time split by fencing status. The blue line represent the estimate from the simple linear model on that treatment level\r\n\r\n\r\n\r\nHowever, this still doesn‚Äôt really represent the experimental design very well, as the data was collected at wetlands that used either press or crash grazing and well as having fenced and unfenced transects. If we update the plot to incorporate grazing type as well (Figure 3) we can see that there may also be a differential effect of grazing type on the fencing effect. This means that we should include a second interaction in our model.\r\n\\[\r\n\\text{max_ht} \\sim \\text{df_survey} * \\text{fence} * \\text{grazing}\r\n\\]\r\n\r\n\r\nShow code\r\n\r\nggplot(ds_ht, aes(dt_survey, max_ht)) + geom_point() +\r\n  stat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\") +\r\n  ylab('Maximum plant height (cm)') +\r\n  xlab('Time') +\r\n  facet_grid(grazing ~ fence) \r\n\r\n\r\n\r\nFigure 3: Plot of maximum plant height at the quadrat over time split by treatment levels. The blue line represent the estimate from the simple linear model on that treatment level\r\n\r\n\r\n\r\nLooking at these plots, you can see there is still a lot of variation in the observed maximum plant height. Some of this maybe due to the variation between wetlands. If we compare the maximum plant height at the Loringhoven and Tsuji wetlands (Figure 4) then you will see that they are much lower and higher respectively than the overall average (Figure 2). This means that we should incorporate wetland into our model as well. However, if we are trying to generalise these results across all similar wetland in these CMAs, then we don‚Äôt really care about the maximum plant height being larger at one wetland compared to another. Instead, we can treat them as a random effect. While some wetlands typically have a greater maximum plant height, others lower and some are average, we don‚Äôt care about them individually, we care about the variation between them. If we include this random effect into our model it will take the form2:\r\n\\[\r\n\\text{max_ht} \\sim \\text{df_survey} * \\text{fence} * \\text{grazing} + (1|\\text{wetland)}\r\n\\]\r\n\r\n\r\nShow code\r\n\r\nggplot(ds_ht %>% filter(wetland %in% c('Loringhoven', 'Tsuji')), aes(dt_survey, max_ht)) + geom_point() +\r\n  stat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\") +\r\n  ylab('Maximum plant height (cm)') +\r\n  xlab('Time') +\r\n  facet_grid(wetland ~ fence, scales = 'free_y') \r\n\r\n\r\n\r\nFigure 4: Plot of maximum plant height at the quadrat over time split by fencing status at the Loringhoven and Tsuji wetlands. The blue line represent the estimate from the simple linear model on that treatment level that wetland\r\n\r\n\r\n\r\nMixed effects models: trying to account for correlation\r\nIf we have a model with both fixed and random effects this model is known as a mixed effects model. These types of models are very useful in trying to account for correlations (structural patterns) in the residuals. These patterns in the residuals can take many forms:\r\nResiduals are the difference between the actual observed response data and the estimated response using the model.\r\nHierarchical model (aka a nested or multilevel model) is where there observations in the same group (or level) are more similar to each other. Think of how the groupings of region, wetland and transect are nested within each other. Observations within the same transect are likely to be more similar than at different transects. However, observations at the same wetland may be more similar than at different wetland.\r\nTemporal correlation is where observations nearer in time are more similar to each other\r\nSpatial correlation is where observations physically near each other are more similar to each other\r\nSpatio-temporal correlation is where observations near each other either physically or in time are more similar to each other\r\nLooking at the results of linear models at the transect level from the Eggeling wetland (Figure 5), it shows the difference between transect. It would be good to be able to account for some of the is extra variation. Similar to the wetland level effects, we ‚Äúdon‚Äôt care‚Äù about the specific transect, as we are looking to be able to generalise across all possible transects. Hence, we will include transect as a random variable, but it is ‚Äúnested‚Äù within wetland, so we will include it as a hierarchical (or nested) random effect. So the model now becomes:\r\n\\[\r\n\\text{max_ht} \\sim \\text{df_survey} * \\text{fence} * \\text{grazing} + (1|\\text{wetland} / \\text{transect_id})\r\n\\]\r\n\r\n\r\nShow code\r\n\r\nggplot(ds_ht %>% filter(wetland == 'Eggeling'), aes(dt_survey, max_ht, color = transect_id)) + geom_point() +\r\n  stat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\") +\r\n  ylab('Maximum plant height (cm)') +\r\n  xlab('Time') +\r\n  facet_grid(. ~ fence, scales = 'free_y') \r\n\r\n\r\n\r\nFigure 5: Plot of maximum plant height at the quadrat over time split by fencing status at the Eggeling wetland. The lines represent the estimate from the simple linear model on that treatment level at that transect\r\n\r\n\r\n\r\nThe underlying distribution\r\nSo far we have implicitly assumed the data is normally distributed and follows all the required assumptions, like the residuals are independent and normally distributed and their variance is constant. However, if you look at Figure 5 you will notice that some of the confidence intervals around the mean and even some estimates are negative. Given that the response variable of interest is maximum plant height where values can‚Äôt be negative, a different distribution should be chosen. Each distribution has a range of values it can take. If is important that as a first pass, the distribution you chose to use in your model does not allow for values outside the scope of your response variable.\r\n\r\nThe normal (or Gaussian) distribution can theoretically take any value from \\(-\\infty\\) to \\(\\infty\\). If your mean is a long way from 0 compared to you standard deviation, then using a normal distribution may not cause any obvious issues.\r\nIn our example where the response variable can take any non-negative values it may make sense to try a log-normal or gamma distribution, each of which can only take positive values. Given the log-normal distribution is easier to use, so we will use it in our model.\r\n\r\n\r\nShow code\r\n\r\nggplot(expand.grid(x=seq(0.01, 100, 0.01), log_mean=c(1, 2.5, 4)) %>% mutate(y=dlnorm(x, log_mean, 1), log_mean=factor(log_mean)), aes(x,y, group=log_mean, color=log_mean)) +\r\n  geom_line() + geom_hline(yintercept=0, color='grey') +\r\n  geom_vline(xintercept=0, color='grey') + ylab('Probability') + scale_color_discrete(name = 'log-mean')\r\n\r\n\r\n\r\nFigure 6: Plot of log-normal distributions with the log-means of 1, 2.5 and 4 and a log-standard deviation of 1\r\n\r\n\r\n\r\nCreating the model\r\nUsing a combination of us exploring the data and the experimental design, it looks like we will try log-normal generalised linear mixed model (glmm). Using the log-normal distribution means that we won‚Äôt predict negative numbers. The interaction of time, fencing and grazing type will be fixed effects and relate to our key question of grazing intensity affecting wetland plants. The wetland and transect could be nested in a hierarchical model of random effects, but for simplicity and quicker convergence we will just use wetland as a random effect. This can account for the repeated measures of sampling the same places over time.\r\nAdvanced topic: Frequentist or Bayesian?\r\n\r\nChances are the statistics you learnt at university were frequentist statistics: t-tests, ANOVA, linear regression and p-values. It relies on long-run probabilities (how probable is this data given the null hypothesis). For many models, parameters can be relatively easily estimated using frequentist statistics, and solutions can be found quickly.\r\n\r\n\r\nInside RStudio/R functions that use frequentist models include the base functions like ‚Äúlm‚Äù and ‚Äúglm‚Äù as well as ‚Äú(g)lmer‚Äù and ‚Äúgam(m)‚Äù from ‚Äúlme4‚Äù and ‚Äúmgcv‚Äù respectively.\r\n\r\nMore recently, Bayesian statistics have come into wider use with modern computing. Bayesian approaches involve the probability of a hypothesis given the dataset. It did require programming to generate models, but over the last 5 or so year, packages that will allow you to run without programming have become available for use. However, due to the number of iterations and complex calculations required, models can take between 5 minutes to 5 days to run and converge.\r\n\r\n\r\nInside RStudio/R packages for Bayesian models include ‚Äúrjags‚Äù and ‚Äúbrms‚Äù, that utilise external programs like jags and stan respectively.\r\n\r\nSome people have a particular adherence to either framework. Some people just use whichever tools will get the job done quickest. At ARI, most people are in the latter camp. If your model is relatively simple, then frequentist models should give you a quick and accurate answer. If your model is more complicated, then it may be simpler to do it in a Bayesian system.\r\n\r\nCoding the model\r\nTo construct the model we are going to use the package lme4 and the function glmer. The code for the function mimics the formulation we used earlier, with the addition of the data source and the distribution used (called ‚Äúfamily‚Äù).\r\n\r\nNote that there are some zero maximum heights in the data. You cannot take the log of zero, so to stop this error we will remove the zeros from the analysis. This has the effect of making the model conditional on there being plants present at the quadrat.\r\n\r\n\r\nmod_ht <- glmer(max_ht ~ grazing*fence*dt_survey + (1|wetland),\r\n                data = ds_ht %>% filter(max_ht > 0),\r\n                family = gaussian(link = \"log\"))\r\n\r\nWarning: Some predictor variables are on very different scales:\r\nconsider rescaling\r\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl =\r\ncontrol$checkConv, : Model failed to converge with max|grad| =\r\n3.43258 (tol = 0.002, component 1)\r\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue\r\n - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio\r\n - Rescale variables?\r\n\r\nUsing the date in the model as a date seems to be causing computational issues. Usually, I scale my numerical predictor variables, so that they have a maximum and minimum value between -5 and 5. Here I am going to instead use the visit number that is give as the last digit in the survey_id. To help with interpretations, I am going to subtract 1 from the visit number, so that the first visit is 0 (pre fencing) and this will mean the intercepts now correspond to the initial mean maximum heights.\r\n\r\n\r\nds_ht_scaled <- ds_ht %>% filter(max_ht > 0) %>% mutate(visit = as.numeric(str_sub(survey_id, start = -1)) - 1)\r\nmod_ht <- glmer(max_ht ~ grazing*fence*visit + (1|wetland),\r\n                data = ds_ht_scaled,\r\n                family = gaussian(link = \"log\"))\r\n\r\n\r\nChecking model assumptions\r\nBefore we look at the output from the model, we should assess the model assumptions were violated. To do this, we can use the package DHARMa. The idea is that by running some simulations, you can get a new distribution for the residuals that should always follow the same pattern if the model assumptions met. That distribution is a flat (or uniform) distribution from 0 to 1. By looking at this or the subsequent QQ plot and residuals versus predicted plots, you will be able to see any deviations from what is expected. Unfortunately, our model seems to have fails some of the model assumptions, as the points and test show deviations from the expected. In particular, the assumption that the change over time is linear looks to have been violated, and there are some large outlier in the final survey.\r\n\r\nThe QQ plots should have all the points on the 45\\(^\\circ\\) line from (0, 0) to (1, 1).\r\n\r\n\r\nsim_out_ht <- simulateResiduals(fittedModel = mod_ht, plot = F)\r\nhist(sim_out_ht)\r\n\r\n\r\nplot(sim_out_ht)\r\n\r\n\r\nplotQQunif(sim_out_ht)\r\n\r\n\r\ntestCategorical(sim_out_ht, catPred = ds_ht_scaled$visit)\r\n\r\n\r\n$uniformity\r\n$uniformity$details\r\ncatPred: 0\r\n\r\n    Asymptotic one-sample Kolmogorov-Smirnov test\r\n\r\ndata:  dd[x, ]\r\nD = 0.25491, p-value < 2.2e-16\r\nalternative hypothesis: two-sided\r\n\r\n---------------------------------------------------- \r\ncatPred: 1\r\n\r\n    Asymptotic one-sample Kolmogorov-Smirnov test\r\n\r\ndata:  dd[x, ]\r\nD = 0.13818, p-value < 2.2e-16\r\nalternative hypothesis: two-sided\r\n\r\n---------------------------------------------------- \r\ncatPred: 2\r\n\r\n    Asymptotic one-sample Kolmogorov-Smirnov test\r\n\r\ndata:  dd[x, ]\r\nD = 0.091456, p-value = 1.192e-11\r\nalternative hypothesis: two-sided\r\n\r\n---------------------------------------------------- \r\ncatPred: 3\r\n\r\n    Asymptotic one-sample Kolmogorov-Smirnov test\r\n\r\ndata:  dd[x, ]\r\nD = 0.14932, p-value < 2.2e-16\r\nalternative hypothesis: two-sided\r\n\r\n---------------------------------------------------- \r\ncatPred: 4\r\n\r\n    Asymptotic one-sample Kolmogorov-Smirnov test\r\n\r\ndata:  dd[x, ]\r\nD = 0.1423, p-value < 2.2e-16\r\nalternative hypothesis: two-sided\r\n\r\n---------------------------------------------------- \r\ncatPred: 5\r\n\r\n    Asymptotic one-sample Kolmogorov-Smirnov test\r\n\r\ndata:  dd[x, ]\r\nD = 0.31127, p-value < 2.2e-16\r\nalternative hypothesis: two-sided\r\n\r\n\r\n$uniformity$p.value\r\n[1] 0.000000e+00 0.000000e+00 1.192468e-11 0.000000e+00 0.000000e+00\r\n[6] 0.000000e+00\r\n\r\n$uniformity$p.value.cor\r\n[1] 0.000000e+00 0.000000e+00 1.192468e-11 0.000000e+00 0.000000e+00\r\n[6] 0.000000e+00\r\n\r\n\r\n$homogeneity\r\nLevene's Test for Homogeneity of Variance (center = median)\r\n        Df F value    Pr(>F)    \r\ngroup    5  42.321 < 2.2e-16 ***\r\n      8526                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nModel interpretation and displying results\r\nLet us pretend for the moment that the model assumptions are not violated. If we look at the output from this model it can be a bit overwhelming. To pull out the key table of values that you may use in a report or paper you can use the function tidy from the broom.mixed package. This shows that many of the terms are significant, but is still a bit confusing to the uninitiated.\r\n\r\n\r\nShow code\r\n\r\ntidy(mod_ht)\r\n\r\n# A tibble: 10 √ó 7\r\n   effect   group    term       estimate std.error statistic   p.value\r\n   <chr>    <chr>    <chr>         <dbl>     <dbl>     <dbl>     <dbl>\r\n 1 fixed    <NA>     (Intercep‚Ä¶  3.83      0.0846     45.3    0       \r\n 2 fixed    <NA>     grazingPr‚Ä¶ -0.683     0.145      -4.71   2.43e- 6\r\n 3 fixed    <NA>     fenceYes    0.00936   0.0238      0.393  6.94e- 1\r\n 4 fixed    <NA>     visit       0.0233    0.00578     4.02   5.72e- 5\r\n 5 fixed    <NA>     grazingPr‚Ä¶  0.230     0.0558      4.13   3.67e- 5\r\n 6 fixed    <NA>     grazingPr‚Ä¶  0.121     0.0138      8.78   1.70e-18\r\n 7 fixed    <NA>     fenceYes:‚Ä¶  0.0263    0.00776     3.38   7.13e- 4\r\n 8 fixed    <NA>     grazingPr‚Ä¶ -0.0458    0.0174     -2.63   8.59e- 3\r\n 9 ran_pars wetland  sd__(Inte‚Ä¶ 10.4      NA          NA     NA       \r\n10 ran_pars Residual sd__Obser‚Ä¶ 29.4      NA          NA     NA       \r\n\r\nAn alternative is to use a package like emmeans to estimate the the marginal means for the variables of interest. We can set-up a simple comparison between fenced and not fenced for each grazing type. It shows that at both grazing types that the unfenced transects had on average lower maximum plant heights.\r\n\r\n\r\nShow code\r\n\r\nem_ht <- emmeans::emmeans(mod_ht, ~ fence | grazing*visit)\r\npairs(em_ht, simple = 'fence')\r\n\r\ngrazing = Crash, visit = 2.26:\r\n contrast estimate     SE  df z.ratio p.value\r\n No - Yes  -0.0688 0.0138 Inf  -4.976  <.0001\r\n\r\ngrazing = Press, visit = 2.26:\r\n contrast estimate     SE  df z.ratio p.value\r\n No - Yes  -0.1956 0.0264 Inf  -7.397  <.0001\r\n\r\nResults are given on the log (not the response) scale. \r\n\r\nTypically, a plot of the predicted values can be more informative than the parameter estimates tables. To produce the estimates from a model is R, typically you use the predict function. This works for glmer models to produce the estimate, but to produce the confidence intervals for those predictions we need to bootstrap the predictions and post-process the confidence intervals using the 0.025 and 0.975 quantiles from the simulations.\r\n\r\nBootstrapping is a technique that simulate the outcome using a random subset of the data in each simulation. The resulting data can be used estimate derived statistics of interest, like standard errors or confidence intervals.\r\n\r\n\r\nShow code\r\n\r\npred_data <- ds_ht_scaled %>% select(fence, visit, grazing) %>% distinct()\r\n# predFun <- function(fit) {\r\n#   predict(fit, newdata = pred_data, re.form = NA)\r\n# }\r\n# boot_results <- bootMer(mod_ht, FUN = predFun, nsim = 1000)\r\n# ci_lower <- apply(boot_results$t, 2, quantile, 0.025)\r\n# ci_upper <- apply(boot_results$t, 2, quantile, 0.975)\r\n# predictions <- predict(model, newdata = your_new_data, re.form = NA)\r\n# results <- data.frame(predictions, ci_lower, ci_upper)\r\nds_pred_ht <- readRDS('data/ds_pred_ht.rds')\r\n\r\n\r\nThe results of the bootstrapping can then be used to produce plots. In Figure 7 it is easy to see that at crash wetlands the average maximum height has increased over time, while there is no significant increase at the unfenced transects. However, at the press wetlands, the change in time at the fenced and non-fenced transects looks similar.\r\n\r\n\r\nShow code\r\n\r\nggplot(ds_pred_ht, aes(visit, est, ymin = lb, ymax=ub)) + geom_line(linetype = 'dashed') +\r\n  geom_pointrange() + facet_grid(grazing~fence) +\r\n  scale_y_continuous(limits = c(0, NA)) + geom_hline(yintercept = 0, color = 'grey') +\r\n  xlab('Number of previous visits') + ylab('Maximum plant height (cm)')\r\n\r\n\r\n\r\nFigure 7: Plot of log-normal distributions with the log-means of 1, 2.5 and 4 and a log-standard deviation of 1\r\n\r\n\r\n\r\nHands-on compenent\r\nBelow is the link to the project files for today\r\n\r\nYour Task: for the less experienced\r\nGo through the steps in the project a make sure they work, and see if you can follow along\r\nIf you feel confident, maybe change some of the options in the plots\r\n\r\nBelow is the link to just the data\r\n\r\nYour Task: for the more experienced\r\nDownload the workshop data\r\nCreate your own model for the relationship to maximum plant height that may address the problems in the model\r\nTest the model assumptions to see if your model is more appropriate\r\nProduce a summary table of the model output\r\nProduce a plot of the predicted values for your model\r\nAnalyses the species richness data using the supplied dataset\r\n\r\n\r\nIn R coding practice, ‚Äú*‚Äù is used to mean a full interaction of the predictors and includes their individual predictors. If you wanted to specify just the interaction, you would use ‚Äú:‚Äù.‚Ü©Ô∏é\r\nIn different packages the random effect is referenced differently. This form common and is that used in packages like lme4 and brms. It means wetland is treated like a factor and only effects the intercept.‚Ü©Ô∏é\r\n",
    "preview": "sessions/day2-analysis-workflow/distill-preview.png",
    "last_modified": "2024-09-17T08:39:21+10:00",
    "input_file": {}
  },
  {
    "path": "sessions/day1-getting-data-into-shape/",
    "title": "Day 1: Getting data into shape",
    "description": "An introduction to workflow and the Tidyverse world",
    "author": [
      {
        "name": "Ben Fanson",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-04",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nDay‚Äôs objectives\r\nKey packages\r\nWorkshop project: Wetland Data\r\nWorkflows\r\nBasic of data analysis workflow\r\nExample of a R/Rstudio workflow\r\nAdvantages of workflow\r\nCaveat on my R/Rstudio workflow\r\n\r\nReady, set, go‚Ä¶.\r\nP1: Setup your project folder\r\nImportant concept of relative paths\r\nLet‚Äôs talk a look at my standard import script\r\nTidyverse\r\nSide note - Version control/GitHub [Advanced topic]\r\n\r\nP2: Importing data\r\nimport philosophy\r\nKey importing functions\r\nImporting our project files (starting with site)\r\n\r\nP3: Data cleaning\r\nOverview of basics\r\ncolumn management\r\n\r\nP4: summarising data\r\nFew other examples\r\n\r\nP5: Restructure datasets\r\nOverview\r\nuse of distinct()\r\n\r\nImporting survey information\r\nMissing values\r\n\r\nP6: Joining datasets\r\nCommentary on merges/join (left_join)\r\nJoin for our project\r\nConcatenating - Stacked joins\r\n\r\nP6: Saving the data files\r\nRDS approach\r\nExcel\r\nCSV/Table\r\n\r\n\r\nHelp!!!!!!!!\r\nUsing AI for coding\r\n\r\nHands-on component\r\nAdvanced topics [Advanced]\r\nQuality check packages\r\nvalidate package\r\npointblank package\r\n\r\nBig Data\r\nLearn functions\r\n\r\nResources\r\n\r\nDay‚Äôs objectives\r\nUnderstand workflow, why it is important, and how to do it using Rstudio/R\r\nHave an understanding of the tidyverse framework and its key packages\r\nProvide a bit of context/framework to working in R as many of us have learned piecemeal\r\nWork through an example\r\nKey packages\r\n\r\nWorkshop project: Wetland Data\r\nClick over to dataset webpage for overview of the wetland data\r\nWorkflows\r\nBasic of data analysis workflow\r\nA basic workflow for data analysis is shown below. We will be following this workflow through the workshop. This workflow is likely familiar everyone. Here, we demonstrate how R (with Rstudio) can be used for every step, so you are using a single tool for every step.\r\n\r\nExample of a R/Rstudio workflow\r\nSo those are the basics and suspect most will be rather familiar with that workflow. But, examples speak volumes. Let‚Äôs go through an example of the fundamentals of my R/Studio workflow (I will show a few of my workflow)\r\nLet‚Äôs review\r\nProject folder structure\r\nhow do you name your project folder so easy to find\r\n\r\nData storage: three main folders raw, rds, spatial\r\nwhat are the key data folders you need for your project\r\n\r\nR scripts : import/data cleaning, analysis\r\n\r\nstructure within the scripts (example of importData.r)\r\n\r\nReport writing: Rmarkdown/Quarto file [day3]\r\n\r\n\r\nRmarkdown used to be the main report tool in R but recently Quarto has been slated to replace Rmarkdown. I am still using Rmarkdown.\r\nAdvantages of workflow\r\n‚Äúlaziness‚Äù in data analysis programming\r\nreduce cognitive load\r\nreduce distractions (rabbitholes/‚Äúthe weave‚Äù)\r\nefficiency\r\n\r\ncollaborations\r\na logical workflow makes for easy collaborations\r\n\r\nreproducibility\r\nlogical, clean workflows minimise error\r\nevery step is explicit in the code [no black boxes]\r\n\r\nCaveat on my R/Rstudio workflow\r\nMy R/Rstudio workflow has evolved over the years and reflects historical contingencies (e.g.¬†working as analyst in pharmaceutical for 9 months, influential people I happened to meet).\r\nThis workshop will work off this framework but this workflow might not fit you per se. However, for those developing their workflow, it is useful to have an initial foundation in which to work off. So, as we go through this workshop, think about how you personalise the workflow for you:\r\nhow might you integrate your current process with the workflow presented?\r\nwhat unique aspects of your work need to be integrated into the workflow?\r\nwhat is the role of collaborators and meshing workflows?\r\nUpfront thinking about this is not time-wasted. If you have a great structure that inherently works for you, you will be more efficient.\r\nReady, set, go‚Ä¶.\r\nOkay, we have the data and understand the project goals, design, and key questions. Let‚Äôs go through the basic steps to get going‚Ä¶\r\nP1: Setup your project folder\r\nIf new to Rstudio, it is worth checking out youtube tutorials (e.g.¬†an example one ). We will try to show tips and tricks along the way.\r\nCreate Rproject using Rstudio\r\n\r\nCreate folder structure\r\n\r\n\r\nNote that ‚ÄúRstudio‚Äù (the company) has rebranded to ‚Äúposit‚Äù. It has also created a new IDE,0 Positron https://github.com/posit-dev/positron/wiki for note about new IDE\r\nCreate an importData.r script\r\n\r\nImportant concept of relative paths\r\nProbably the most important attribute of an Rstudio project is the idea of relative path. In your analysis, since you are using R code, you will need to point to files using file paths (e.g.¬†‚Äúdata/raw/data.xlsx‚Äù).\r\n\r\n\r\n# you can see the working directory by typing\r\n  getwd()\r\n\r\n\r\n\r\nNotice that I used forward slash ‚Äú/‚Äù instead of backslash ‚Äú‚Äú. In R, you need to use forward slash in file paths as backslash are special‚Äùescape‚Äù characters\r\nAn absolute path is the full path ‚ÄúC:/Users/bf0a/workshops/2024_WetlandGraze/data/raw/data.xlsx‚Äù. If you use absolute path, any time you change your computer or share your project across computers, you will need to change the path. A relative path means that you just need to specify the path from within the project folder.\r\nGiven that, it is important to open your project using the ‚Äúproject_name.Rproj‚Äù link, e.g.¬†\r\nLet‚Äôs talk a look at my standard import script\r\n\r\n\r\nClick the outline button in upper right of the script screen to show the outline of script. The outline is created by the #### Text ####\r\nTidyverse\r\nAt the top of my script, I put library(tidyverse)\r\n\r\n\r\nTidyverse is big topic but here are key points that I want you know:\r\nTidyverse is an opinionated approach to data analysis and has spurred a movement of tidy-everything (tidygraphs, tidybayes, sf package, and so much more)\r\nRather than a package per se, it is really a collection of packages (aka meta-package)\r\nTidyverse goals\r\nmake R code more consistent\r\nmake R code more readable for humans\r\ncreate a cult\r\n\r\nA few key functions\r\nThere are lot of functions in tidyverse world. Below, I will just quickly introduce a few to show the human-readable attribute and give you your first exposure to what we will use later. These functions fall (very) loosely into three categories (though not mutually-exclusive): 1) dataset cleaning tools; 2) summarising; and 3) dataset reshaping/joins.\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n# dataset cleaning tools\r\n  select()  # choose the columns you want [subset by column]\r\n  filter()  # choose the rows you want [subset by row]\r\n  arrange() # sort the columns\r\n  mutate()  # create a new variable\r\n\r\n# dataset summarising tools\r\n  group_by()    # implicitly breaks the dataset into separate groups\r\n  summarise()   # performs a summary statistic on each individual group\r\n\r\n# dataset reshaping and joining   \r\n  left_join()   # combining columns from two datasets\r\n  bind_rows()   # stacking dataset on top of each other\r\n\r\n\r\n\r\n\r\n\r\nPipes: %>%\r\nA key component of tidyverse world is the concept of ‚Äúpipes‚Äù. The pipe connects multiples steps into a single sequence of commands.\r\n\r\nJust to make things harder, a new pipe |> has been introduced. It is very similar to %>% but it does act a little different. We will just teach %\\>% because we are set in our ways\r\nBelow is an example of building a sentence. You start with a dataset (or other object) and then do things to that dataset, passing (%>%) onto the next thing thing.\r\nIn Rstudio, there is shortcut: ctrl + shift + M to add a pipe‚Ä¶use it!!!!.\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n  library(tidyverse)\r\n  head(mtcars,10)   # mtcars is a built in dataset\r\n  mtcars %>% head(10)  # get mtcars, pass to head() function, take the first 10 rows \r\n\r\n# sentence using tidy functions we will dive into below...just follow basic logic\r\n  mtcars %>% \r\n    rename( cylinders = cyl ) %>% # chagne cyl to cylinder\r\n    mutate( mpl = round( mpg/3.79,2 ) ) %>%  # convert mpg to miles per liter (mpl)\r\n    select( mpg, mpl, cylinders )  # keep just the mpg and mpl and cylinders columns \r\n\r\n  #vs.\r\n  ds_m <- rename(mtcars, cylinders = cyl )          # change cyl to cylinder\r\n  ds_m <- mutate(ds_m, mpl = round( mpg/3.79,2 ) )  # convert mpg to miles per liter (mpl)\r\n  select(ds_m, mpg, mpl, cylinders )                # keep just the mpg and mpl and cylinders columns \r\n\r\n\r\nSide note - Version control/GitHub [Advanced topic]\r\nAs part of my due-diligence, I have to mention version control. In short, version control is external software that keeps track of all your changes (so you can go back if something breaks in your code) and very helpful with collaborations (e.g.¬†allowing you both code away and then merging those changes).\r\nFor R/Rstudio, Git/GitHub is the most popular (and can be directly integrated). Now, Git is the version control software locally on your computer and it does the tracking of all changes In contrast, GitHub is an online platform in which you can upload those changes (and project folder/files) and is mainly useful for the collaboration/sharing (plus some other useful features)\r\nKey points on Git/GitHub:\r\nIf not collaborating, the overhead of Git (learning, initial setup, random breaks) might not worth it to you. You still have version histories via Onedrive to save you (not as easy to shift through as Git).\r\nIf collaborating, it really is the best approach that will save you effort in the long run.\r\nIt is worth playing around with GitHub online so you know how to navigate the website (this workshop will help with that). GitHub is a rich resource with example code and useful packages not on CRAN. GitHub project (aka repositories) can look intimidating at first.\r\nRstudio has git integration that makes it easier to work with, though the github desktop\r\nGitHub has GitHub pages which is hosting this website [workflow: 1) write RMD files in Rstudio; 2) upload to GitHub; 3) GitHub publishes at https:username.github.io/project_id]\r\nResearchers will host their packages on GitHub instead of CRAN\r\n\r\nCRAN is repository for packages and is teh default place to look for a package. As CRAN has certain rules/time-lags, some by-pass using other repositories like GitHub\r\nP2: Importing data\r\nAfter setting up environment, on to the import stage\r\nimport philosophy\r\nIn my workflow, I view data files as ‚Äúlocked(/read-only)‚Äù. This is part of the reproducibility pathway. So this means for me:\r\nI do not change filenames once the data go in that folder [e.g.¬†if you do, you will forget where your file came and have to go digging]\r\nI will not touch anything in the files.\r\nEither I code around (so anyone can see the change)\r\nSend back to get it fix\r\n\r\nKey importing functions\r\nexcel files\r\nFor this, we will use readxl package\r\n\r\nThere are other read excel packages but this one just works\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n  library(readxl)\r\n  read_xls('data/raw/example.xlsx')\r\n  read_xlsx('data/raw/example.xlsx')\r\n\r\n\r\nYou should be aware of how it handles missing data. In read_xlsx() you can specify an argument na= to indicate what cell values should be treated as missing.\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n  library(readxl)\r\n  read_xlsx( 'data/raw/missing.xlsx', na='' ) # default\r\n\r\n# A tibble: 5 √ó 2\r\n    var missing\r\n  <dbl> <chr>  \r\n1     1 NA     \r\n2     2 na     \r\n3     3 .      \r\n4     4 <NA>   \r\n5     5 Na     \r\n\r\n  read_xlsx( 'data/raw/missing.xlsx', na=c('NA','na') )\r\n\r\n# A tibble: 5 √ó 2\r\n    var missing\r\n  <dbl> <chr>  \r\n1     1 <NA>   \r\n2     2 <NA>   \r\n3     3 .      \r\n4     4 <NA>   \r\n5     5 Na     \r\n\r\ncsv/txt\r\nFor reading csv/txt\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n# from readr package in tidyverse\r\n  read_csv('data/raw/example.csv')    # comma delimited\r\n  read_table('data/raw/example.txt')  # tab delimited\r\n  read_delim('data/raw/example.ext', delim='#') # or other type of delimited \r\n  \r\n  ?read_csv  # to see arguments/options for importing\r\n\r\n\r\nDatabase [Advanced]\r\nSome possible packages to check:\r\nRMySQL: This package allows you to connect to MySQL databases and execute SQL queries1.\r\nRPostgreSQL: This package is designed for connecting to PostgreSQL databases1.\r\nRSQLite: This package is used for SQLite databases and is great for lightweight, serverless database applications1.\r\nRODBC: This package provides a way to connect to any database that supports ODBC (Open Database Connectivity)1.\r\nRJDBC: This package uses Java Database Connectivity (JDBC) to connect to a wide variety of databases1.\r\nImporting our project files (starting with site)\r\n\r\n\r\n#_____________________________________________________________________________\r\n#### Global Variable/Settings ####\r\n  library(tidyverse)  # this brings in all the tidyverse packages\r\n  library(readxl)     # we will need this \r\n\r\n  file_excel <- 'data/raw/Workshop 2024 data.xlsx'   # created new variable to reference\r\n  \r\n#_____________________________________________________________________________  \r\n#### Section 1: site information ####\r\n# check sheet names in excel file\r\n  excel_sheets(file_excel)\r\n  \r\n  # bring in site data      \r\n  ds_site <- read_excel(file_excel, sheet = 'site_info')\r\n  \r\n  # take a look\r\n  head(ds_site)  # quick peak\r\n  View(ds_site)  # check out a datatable version...can also just click the object in the \r\n                 # environment panel in upper right\r\n  names(ds_site)  # have a look at object names\r\n\r\n\r\nP3: Data cleaning\r\nOverview of basics\r\nNow we have the first file in. That is the easy bit. However, we need to clean up dataset(s) so that they are ready for analysis. By cleaning I mean:\r\ncolumn management\r\nrenaming columns for coding\r\ndrop extra columns and extra rows\r\nmakes sure column types (e.g.¬†numeric, character, factors, date) are correct\r\ndata entry errors in columns\r\nstring manipulations (e.g.¬†combine two columns, split two columns)\r\n\r\ncheck design (aka get intimate ‚ù§Ô∏è with the data)\r\nextract out the design elements to confirm it matches (or identity where is does not)\r\nmissing data [part of design missing?]\r\n\r\njoin together response variables, design, and covariates\r\nrestructure data for analysis (e.g.¬†long vs wide form)\r\ncolumn management\r\ndropping/keeping columns - select()\r\nthe select() is used to select the columns you want. Here, we use it to drop the two columns we do not. The ‚Äò‚Ä¶10‚Äô was created automatically by read_excel() and not needed. We delete the columns using (-) negative in-front of the column we want to drop. We also learn if a column name has whitespace (or other special characters like %, #, ‚Ä¶) you need to use quotes to refer to it.\r\n\r\n\r\n  # let's delete that the extra column... use select(data, -var)\r\n  ds_site <- select( ds_site, -...10)  # drop the ...10\r\n#  ds_site <- select( ds_site, -Drop me)  # this does not work\r\n  ds_site <- select( ds_site, -\"Drop me\")  # need to use either \"Drop me\" or `Drop me` if you have whitespace\r\n\r\n\r\nrenaming columns - rename()\r\nYou will want to rename columns. It is important to think about what makes a good column name for you. I stick to the following\r\nno whitespaces, no special characters, not too long, (for me) all lowercase\r\nprefixes or suffixes (useful for select functions) [e.g.¬†dates are dt_, datetime is dttm_, site info is site_)\r\n\r\n\r\n  # now, let's rename [I will show two ways: rename() and within select() ]\r\n  ds_site <- ds_site %>% \r\n    rename( site_lat = Transect.Start.Latitude,\r\n            site_lon = Transect.Start.Longitude ) %>% \r\n    select(cma, wetland=wetland_name, transect_id, fence, grazing, starts_with('site_') )\r\n\r\n\r\ncheck column type and convert - mutate()\r\nWhen you want to create a new column or modify an existing column, mutate() is the key function. We used it to convert lat/lon from character to numeric.\r\n\r\n\r\n # now, let's check column type\r\n  head(ds_site)  # <chr> = character\r\n  \r\n  # convert lat/lon to numeric...need to use mutate and will use as.numeric()\r\n  ds_site <- mutate(ds_site, \r\n                    site_lon_n = as.numeric(site_lon),\r\n                    site_lat_n = as.numeric(site_lat) ) # note the warning\r\n  filter(ds_site, is.na(site_lon_n)|is.na(site_lat_n) )  # warning cause it was missing..will get later from researcher\r\n\r\n  # now, I want to keep the site_lat/site_lon column but as numeric \r\n  ds_site <- select(ds_site, -site_lon, -site_lat) %>% \r\n              rename( site_lat = site_lat_n, site_lon = site_lon_n )\r\n  head(ds_site) # looking good\r\n\r\n\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n# commmon conversion functions\r\n  as.numeric()\r\n  as.character()\r\n  as.factor()\r\n  as_date()  # note there is as.Date() and as_date()...use the later\r\n  as_datetime()  # note there is as.Date() and as_date()...use the later\r\n\r\n\r\nFor dates, this is another whole topic. I highly recommend you use the lubridate package for all date handling. See date webpage for a bit more info\r\nFor factors, be very careful when converting‚Ä¶\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n# factors are in essence an ordinal in that they are stored as numbers with labels\r\n  intensity <- c('low','medium','high') \r\n  intensity\r\n  class(intensity) # character\r\n  f_intensity <- as.factor(intensity)\r\n  f_intensity # now a factor and note the levels\r\n  as.numeric(f_intensity) # you get numbers\r\n\r\n# here is where the problem comes in...\r\n  ds_fish  <- data.frame( fish_id = c(1,2,6,7) ) # say you have fish id as a number\r\n  ds_fish  <- mutate(ds_fish, f_fish_id = factor(fish_id) ) # you convert fish_id to a factor for a model  \r\n  mutate(ds_fish, fish_id = as.numeric(f_fish_id) ) # you want to join it to another dataset and it needed to be numeric, you use as.numeric() you get the wrong numbers\r\n  # note - this use to be a bigger problem pre-tidyverse0\r\n\r\n\r\ndata issues and using filter()\r\nNext, we checked each column of interest to see if it matched our expectations. We introduced a string function str_to_sentence() to help. You also saw filter() and how to use that. The operators for fitler() are ==, !=, %in%. <aside>There are tons of string functions. Check the cheatsheet.\r\n\r\nWe also learned about pull(data, var) vs data$var way of getting a specific column from the dataset\r\n\r\n\r\n# first, you need to learn to grab a single column from a dataset\r\n  pull(ds_site, cma)  # this is tidyverse way\r\n  #vs\r\n  ds_site$cma  # this is the old base way\r\n  \r\n  # I will use $ method here as this is more common (or maybe I am just old(-school) )\r\n  table(ds_site$cma) # looks good\r\n  \r\n  table(ds_site$wetland) # looks good, no misspellings obvious, numbers not exactly 10\r\n  \r\n  table(ds_site$fence) # oops, yes vs Yes...capitalisation matters\r\n  ds_site <- mutate( ds_site, fence = str_to_sentence(fence) )  # let's use a string function\r\n  table(ds_site$fence) # Too easy!\r\n  \r\n  table(ds_site$grazing) # okay, was excepting just Crash/Press - was not expecting\r\n  ds_site <- filter(ds_site, grazing != 'NIL' )  # ! is used to say not\r\n  ds_site <- filter(ds_site, grazing %in% c('Crash','Press' ) )  # or you inverse way \r\n  table(ds_site$grazing) # all good\r\n\r\n\r\nP4: summarising data\r\nAfter you did initial column cleaning, we explored the experimental design to make sure that it was as expected. We found several issues and fixed them. We covered group_by() and combining with count(), summarise(), and mutate(). The key difference between summarise and mutate is that summarise returns a single row per group whereas mutate returns all initial row, adding in a the new variable.\r\nOnce grouped, the data stays that way until you ungroup() the data. This will trick you up!!!! If you do a calculation and cannot figure out why it is very wrong, check if data are still grouped. It is best to always ungroup() but that is not reality.\r\n\r\nIf you want to take the first(/last) number of rows, check out the family of slice() functions in dplyr. See the dplyr cheatsheet for examples\r\nAlso, we saw a ‚Äòspecial‚Äô function n() that you can use to call to get the number of rows in a group. Another useful function is row_number() which will give you the row number within each group (or the whole dataset is ungrouped).\r\n\r\n\r\n##### let's check experimental design #####  \r\n  \r\n  # Now, I want to understand this transect_id.  Is it unique?\r\n  # option1 - if unique, then there will be one per wetland\r\n  ds_dups <- group_by(ds_site, wetland, transect_id) %>% count(name='n')  # count() \r\n  filter(ds_dups, n > 1)  # there are 8 were it is not\r\n  \r\n  # okay, not expecting that...let's add in the number of duplicates so can understand better\r\n  ds_site <- group_by(ds_site, wetland, transect_id) %>% \r\n              mutate( n_dups = n() )  # we use mutate here so it keeps all rows\r\n  filter(ds_site, n_dups==2)          # both No and Yes\r\n  filter(ds_site, n_dups==2) %>% distinct(wetland)  # in three wetlands...\r\n    # in the above code, I use distinct() to return the unique level for wetland\r\n    # but as you saw, it returned transect_id as well.  This is because dataset is still grouped\r\n  ds_site <- ungroup(ds_site)\r\n  filter(ds_site, n_dups==2) %>% distinct(wetland)  # in three wetlands...\r\n  \r\n  # I was told that this was a data entry error and I should drop the Fence==No ones \r\n  filter(ds_site, n_dups==2 & fence=='No')  # let's see the ones to drop..n=8\r\n  nrow(ds_site)\r\n  ds_site <- filter(ds_site, !(n_dups==2 & fence=='Yes') ) # \r\n  nrow(ds_site)  # 300-8 = 292...all good\r\n     \r\n # I want to know how many transects in each...let's use group_by() and summarise()\r\n  ds_sum <- ds_site %>% \r\n              group_by(wetland, fence) %>%  # break up into separate wetlands and do a calculation on each group\r\n              summarise( n_transects = n() )\r\n  filter(ds_sum, n_transects!=5)  # huh, thought there were only 5, check with researcher why\r\n\r\n\r\nFew other examples\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n  mtcars # built in dataset\r\n  group_by(mtcars, cyl) %>% summarise( mean_mpg= round(mean(mpg),1), \r\n                                       sd_mpd= round(sd(mpg),1),\r\n                                       n = n() ) # n() is built in to count rows\r\n  # if you replace the same variable, all next calculation will be on the summarised variable\r\n  group_by(mtcars, cyl) %>% summarise( mpg= mean(mpg) %>% round(2), \r\n                                       max_mpg= max(mpg)%>% round(2),\r\n                                       min_mpg= min(mpg)%>% round(2) ) # see how min/max same\r\n\r\n\r\nP5: Restructure datasets\r\nOverview\r\nRestructuring is like a pivot table in Excel. It is going from wide to long (and vice versa). In our script, we used pivot_wider(). The reverse function is pivot_longer() which we did not use here.\r\n\r\n\r\n# I want to know the transect number differ between plot or are equal...restructure\r\n  ds_sum_wide <- pivot_wider( ds_sum, names_from=fence, values_from=n_transects )\r\n  head(ds_sum_wide)\r\n  filter(ds_sum_wide, No != Yes )  # might be worth double-checking if supposed to be\r\n\r\n\r\nuse of distinct()\r\nWe finished off Section 1 by using the distinct() [so more]. distinct() is really useful for extracting out design. distinct() finds the unique combination of all the variables listed in the function. We used to get the unique combination of wetland, grazing, fence. You also saw n_distinct() which is useful in mutate to count the number of unique levels in variable.\r\n\r\n\r\n# let's check that the number wetland per cma\r\n  group_by(ds_site, cma) %>% summarise( n_wetland = n_distinct(wetland) ) # option 1\r\n  distinct(ds_site, cma, wetland) %>% group_by(cma) %>% count()  # options 2\r\n  \r\n  # let's check full design using some of the techniques above\r\n  ds_design <- distinct(ds_site, wetland, grazing, fence) %>% \r\n                group_by(wetland) %>% \r\n                  summarise(n_grazing=n_distinct(grazing), \r\n                         n_fence = n_distinct(fence) )\r\n  # any \r\n    filter( ds_design, n_grazing>1 ) # nothing\r\n    filter(ds_design, n_fence!=2) # nothing\r\n  \r\n# we have wetland, grazing, fence, transect_id but missing temporal component\r\n\r\n\r\nImporting survey information\r\nMissing values\r\n\r\nNA can be tricky. In statistical models, any row with NA will be dropped quietly. When using functions like mean(), sd(), NAs will return NAs.\r\nA new topic learned in Section 2 was NAs. We used the is.na() function to filter out this cells. NA are special characters and you cannot just do filter(ds_site, wetland==NA) or filter(ds_site, wetland=='NA'). The former statement caused R to look for a variable called ‚ÄòNA‚Äô and the latter makes R look for variable level ‚ÄòNA‚Äô.\r\n\r\n\r\n\r\n#_____________________________________________________________________________  \r\n#### Section 2: import survey info ####\r\n  ds_survey <- read_csv('data/raw/df_survey_info.csv') \r\n  ds_survey\r\n  \r\n  ds_survey <- ds_survey %>% \r\n                  select(-...1) %>% \r\n                  rename( dt_survey = date ) # I prefer dt_xxx for my dates\r\n  # check columns\r\n  ds_survey  # all <chr> except for <date> as expected\r\n  summary(ds_survey) # missing dates for dt_survey\r\n  \r\n  # let's check out the NAs...using is.na() function\r\n  filter(ds_survey, is.na(dt_survey))  # ...check with researcher\r\n  ds_survey <- filter(ds_survey, !is.na(dt_survey) )  # let's say they should be deleted\r\n\r\n  # I would check if any transects not in the site dataset\r\n  filter(ds_survey, !(transect_id %in% ds_site$transect_id) )\r\n  filter(ds_site, wetland == 'Hoch') # yep no wetland that is Hoch...oh that is right, it is the NIL we dropped\r\n  \r\n  filter(ds_survey, str_detect(transect_id,'Hoch')) # we want to drop these\r\n  ds_survey <- filter(ds_survey, !str_detect(transect_id,'Hoch')) # drop them\r\n\r\n  # okay, I want to check out dates \r\n  ggplot(ds_survey, aes(dt_survey)) + geom_dotplot()  # showing a graph...matches our expectation (no odd date)\r\n\r\n  # survey dataset is looking good\r\n  #now let's join survey info and site info \r\n  # first, I want to create wetland in the survey dataset\r\n  # I noticed an NA in graph\r\n  ds_survey <- separate_wider_delim(ds_survey,\r\n                                    cols = transect_id,\r\n                                    names=c('wetland','transect_no'),\r\n                                    delim='_',\r\n                                    cols_remove=F)\r\n  ds_survey \r\n  ds_survey <- ds_survey %>% select(-transect_no) # not needed at this point\r\n  \r\n  # second, check that there are not duplicates for survey_id, dt_survey\r\n  group_by(ds_survey, survey_id, dt_survey) %>% count() %>% filter(n>1) # yippe, all clean\r\n\r\n\r\nP6: Joining datasets\r\nmerges/combining joins\r\n\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n# whole suite joins....\r\n  left_join()  # stick this one\r\n  right_join() # just switch around to left_join\r\n  full_join()  # safer to create the fuller design (e.g. expand.grid) and then left_join() into it\r\n  inner_join() # safer to do left_join() and then filter()\r\n  outer_join() # just stick to left_join and filter()\r\n\r\n\r\nCommentary on merges/join (left_join)\r\nYou will join datasets all the time. In joins, (usually) you have two datasets and have something in common (e.g.¬†site_id). For instance, you may have response variable in one dataset and covariates in another. You want to combine together.\r\nBe warned, joins can be major pitfalls!!!! If you are not intimate ‚ù§Ô∏è with your datasets, it can lead to big issues. We showed this in our code in which we ‚Äúforgot‚Äù to include transect_id:\r\n\r\n\r\n# as an example, left_join will warn you if it is not a one-to-one join\r\n  ds_mistake <- left_join( ds_survey, select(ds_site, wetland, fence, grazing) )\r\n  nrow(ds_mistake) # way bigger\r\n\r\n\r\nWhat happened here is called a one-to-many join. I was expecting the left dataset (the first dataset) to have the same number of rows before and after the join. Instead, it has more, creating fake data (repeated) in essence (and increasing sample size, decreasing SEs, leading to erroneous stats). Therefore, you really need to keep tabs on number of rows.\r\nLuckily, dplyr::left_join() brought in a warning for this (I had my own function to warn me about this). My advice for you is to only use left_join() and make it a one-to-one join. Be careful being fancy in one-to-many and many-to-many (full_join) joins. Sometimes it is safer to work in separate steps rather than one big step.\r\nJoin for our project\r\nNow that is off my chest, let‚Äôs recall the joins we needed for our project‚Ä¶\r\n\r\n\r\n# now join in grazing, fence info...use a left_join and join by wetland, transect_id\r\n  nrow(ds_survey)\r\n  ds_survey <- left_join( ds_survey, \r\n                          select(ds_site, wetland, transect_id, fence, grazing) )\r\n  nrow(ds_survey) # same number as before, all good\r\n  \r\n  # as an example, left_join will warn you if it is not a one-to-one join\r\n  ds_mistake <- left_join( ds_survey, select(ds_site, wetland, fence, grazing) )\r\n  nrow(ds_mistake) # way bigger\r\n  \r\n#_____________________________________________________________________________  \r\n#### Section 3: import richness ####\r\n  excel_sheets(file_excel)\r\n  ds_richness <- read_excel(file_excel, sheet='spp_rich' )\r\n  ds_richness  # column types look good\r\n  \r\n  # only keeping native\r\n  ds_richness <- select(ds_richness, survey_id, native)\r\n  \r\n    \r\n  # let's add richness to ds_survey\r\n  # going to do a few checks\r\n  filter(ds_richness, !(survey_id %in% ds_survey$survey_id) ) # all good!\r\n  \r\n  # do a histogram/density plot\r\n  ggplot(ds_richness, aes(native)) + geom_histogram() + \r\n    geom_density(aes(y=after_stat(count) ),alpha=0.2, fill='red') # no obvious outliers at this point\r\n\r\n  # add richness to ds_survey \r\n  ds_richness_out <- left_join(ds_survey, \r\n                         ds_richness)\r\n  ds_richness_out  \r\n  \r\n  filter(ds_richness_out, is.na(native)) # 46 rows of NA...check with researcher\r\n\r\n\r\nConcatenating - Stacked joins\r\nI did not use concatenating in this dataset. A common use for this is when you have multiple files produced that have the same data structure (e.g.¬†acoustic data from loggers, camera trap data, acoustic moth data) and you need to stack them together for the analysis. You will use bind_rows() for that.\r\n\r\n\r\n#### example below - do not include in your hands-on section script ####\r\n\r\n  ds1 <- data.frame( plot='plot1', quadrat=1:2, species_richness=c(10,20), comments=c('none','something'))\r\n  ds1\r\n\r\n   plot quadrat species_richness  comments\r\n1 plot1       1               10      none\r\n2 plot1       2               20 something\r\n\r\n  ds2 <- data.frame( plot='plot2', quadrat=1:2, species_richness=c(2,4))\r\n  ds2\r\n\r\n   plot quadrat species_richness\r\n1 plot2       1                2\r\n2 plot2       2                4\r\n\r\n  bind_rows(ds1, ds2)  \r\n\r\n   plot quadrat species_richness  comments\r\n1 plot1       1               10      none\r\n2 plot1       2               20 something\r\n3 plot2       1                2      <NA>\r\n4 plot2       2                4      <NA>\r\n\r\n\r\nthere is a bind_cols() to add columns next to each other. I rarely use, preferring to use a left_join() to make sure everything lines up\r\nP6: Saving the data files\r\nFinally, we have sparkly-clean datasets (and hopefully no hidden dirt under the rugs). The final step in the importData.r is to save in the data/rds to be used for the next step: modelling!\r\nRDS approach\r\nAt the end of the import and clean stage, I save the dataset(s) as an RDS file. The advantage of this method is that you are saving the dataset(s) in native R format so when you bring back in, there is no conversion.\r\n\r\n\r\n#_____________________________________________________________________________  \r\n#### Section 5: save datasets ####\r\n    \r\n  saveRDS(ds_richness_out, 'data/rds/ards_richness.rds')    \r\n  saveRDS(ds_ht_out, 'data/rds/ards_height.rds')    \r\n\r\n\r\nExcel\r\nYou may want to save an excel file to share. You will need the writexl package for this.\r\n\r\n\r\n# excel file\r\n  writexl::write_xlsx( ds_richness, 'data/export/ards_richness.xlsx') \r\n\r\n\r\nCSV/Table\r\nOr if you prefer low overhead files (smaller size).\r\n\r\n\r\n readr::write_csv( ds_richness,file='ards_richness.csv')\r\n readr::write_delim( ds_richness,file='ards_richness.txt',delim = '\\t') # tab\r\n\r\n\r\nHelp!!!!!!!!\r\nWhen learning and using R, you will get stuck and need help. There are variety of help sources we use on a daily basis.\r\nBuilt-in to Rstudio are help files. This provides an initial starting place if you are interested in how to use a specific function. You just need to put ‚Äú?‚Äù before the function and press run: ?mean() or use the Help tab\r\nGoogle search (e.g.¬†stackoverflow, blogs) can be really helpful for finding new functions, packages, etc.\r\nBiometrics - happy to provide guidance/tips/thoughts\r\nARI‚Äôs QEARI - hacky hour or Teams chat\r\n\r\nAI is becoming more and more useful [I am finding the most useful]\r\nUsing AI for coding\r\nA few AI I use regularly:\r\nCopilot (Microsoft): allrounder AI which we have a free subscription at ARI. It does a decent job [I use this one daily]\r\nChatGPT: another allrounder similar to Copilot\r\nClaude: prefer this one for writing text but seems to work well too\r\nCopilot-GitHub: paid subscription but supposedly really good for programming\r\nFor those not regularly using AI, there are a few things that can help when running your searches for help:\r\nwhen asking for an example, include lots of specific to get closer to what you want. You may want to tell it to use only tidyverse functions for the example code. I often ask it to use a built-in dataset for the example so that I can check it right away.\r\nExperiment and Iterate: Don‚Äôt be afraid to experiment with the code generated by AI. Modify it, run it, and see what happens. If something is not working, you can ask point out the error and see if it can fix it\r\nUse AI for Debugging: If you encounter errors, AI tools can help you debug your code. They can suggest fixes and optimizations, making the debugging process less daunting.\r\nGive it try yourself copilot - type‚Ä¶‚Äúin R and using mtcars dataset show me how to get the mean mpg by each cyl level?‚Äù - if it used the aggregate() function, type the followup: ‚Äúcan you use tidyverse functions instead?‚Äù\r\n\r\nHands-on component\r\n\r\n\r\n Download project data\r\n\r\n\r\n\r\n\r\nYour Task: Create your Rproject for the workshop and import data for analysis-ready datasets\r\nDownload the workshop data (refer to dataset webpage for background info if needed)\r\nSetup your workflow\r\nthink about your naming convention for the project names (does it have year, projectID, client). [My standard is ‚Äú{Year}_{Researcher}_{Topic}‚Äù]\r\ncreate Rproject\r\ncreate folder structure (how do you want yours to look?)\r\n\r\nadd in datasets to your data folder [e.g.¬†data/raw/]\r\ncreate your import script file [add to your script folder]\r\nwhat is your naming convention for R script (e.g.¬†importData.R, 01_import.R, bringInData.R)\r\ncreate your R script structure (e.g.¬†do you have headers, bookmarks)\r\n\r\nfill in script for what we have done so far\r\ngo through this ‚Äúlecture‚Äù and grab script and paste into your script\r\ntest it along the way - paste in, highlight and run\r\n\r\nfinally, write your own script to bring maximum plant height tab\r\nimport plant height data from excel file\r\nclean the data\r\nmerge into the species richness dataset\r\nsave as a plant height RDS file (and try writexl)\r\n\r\nCheck compare your version to my using link below\r\n\r\n\r\nBelow is the link to my Rproject version:\r\n\r\n\r\n Download Rstudio project\r\n\r\n\r\n\r\n\r\nPermutation for Newbies\r\nIf your are new to R, you may have found the material a bit overwhelming. If so, it may be best to just download the Rproject that I have created for today and work your way through that.\r\nSteps:\r\nDownload the Rproject folder (see button above)\r\nSave in your preferred location on your laptop\r\nOpen the project by double clicking using the file with the .Rproj extention\r\n\r\nOnce in Rstudio, open the r/import.R file\r\nRun through the script highlighting each section and clicking Run button (or Ctrl+Enter shortcut)\r\n\r\nAdvanced topics [Advanced]\r\nQuality check packages\r\nIf you know that you will be updating the import of files repeatedly, it can be worthwhile to do some upfront QC/QA checks to ensure that the data coming in is clean. There are some packages to help with that.\r\nA few things to consider with these QA/QC:\r\nAre your data coming a database that already implements QA/QC [ideal world]\r\nAre you working with large datasets (lots of columns, separate tables)\r\nIs this an ongoing project so you will be updating regularly the analyses\r\nIs there an automation process [press run and get outputs at the end]\r\nIf you working with clean data or once off datasets, it may be best to just write your data cleaning checks as we have done here. The overhead of these packages unlikely to outweigh the added benefits. Once you are automating/updating dataset, the overhead is often easily outweighed.\r\nHere are two packages to have a look at‚Ä¶\r\nvalidate package\r\nYou can find an overview of this package on its github page\r\nThe validate package is intended to make checking your data easy, maintainable, and reproducible.\r\npointblank package\r\nYou can find an overview of this package on its github page\r\nThis package takes an data.frame, runs through a bunch of checks on each column and provides an useful report highlighting errors and warning (might be an error).\r\nThe basic steps are below in their graphic:\r\n\r\nMy thoughts: This package seems like a step-up in level of QA/QC from validate. I have used a few times but have not regularly implemented due to overhead. Probably becuase my brain is full, it takes a bit of re-leaning each time and I have not taken the time to create a template that minimises that effort.\r\nBig Data\r\nWhen data starts to get big (millions of row), tidyverse can struggle with speed. This is huge topic but will direct you to a couple useful packages that allows you to just learn dplyr but use other quicker processes\r\ndtplyr package\r\nunder the hood it uses the data.table package\r\nbut you write code in tidy/dplyr\r\n\r\ndbplyr package [if working with a database]\r\nhas the database do all the processes and return the result back to R\r\ntakes advantage of cloud computing\r\n\r\nLearn functions\r\nA fundamental skill in coding is creating functions. Packages are basically just collection of functions. I write functions all the time. This helps keep code clean, are reusable (so saves time), helps prevent cut/paste errors, and improves readability/reproducibility. Definitely worth learning as some point.\r\n\r\n\r\nmy_plus_function <- function(x,y){\r\n    x + y\r\n}\r\n\r\nmy_plus_function(1,2)\r\n\r\n\r\nResources\r\nObviously, we have only scratched the surface of the topics here. Here are couple books I have used but lots out there.\r\nR for Data Science\r\nAdvanced R\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-09-16T14:38:33+10:00",
    "input_file": {}
  }
]
