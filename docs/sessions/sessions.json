[
  {
    "path": "sessions/day1-getting-data-into-shape/",
    "title": "Day 1: Getting data into shape",
    "description": "Tidyverse world",
    "author": [
      {
        "name": "Ben Fanson",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-04",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nDay’s objectives\r\nKey packages\r\nWorkshop project: Wetland Data\r\nWorkflows\r\nBasic of data analysis workflow\r\nExample of a R/Rstudio workflow\r\nAdvantages of workflow\r\nCaveat on my R/Rstudio workflow\r\n\r\nReady, set, go….\r\nP1: Setup your project folder\r\nTidyverse\r\nSide note - Version control/GitHub [Advanced topic]\r\n\r\nP2: Importing data\r\nimport philosophy\r\nKey importing functions\r\nImporting our project files\r\n\r\nP3: Data cleaning\r\nOverview of basics\r\ncolumn management\r\n\r\nP4: summarising data\r\nFew other examples\r\n\r\nP5: Restructure datasets\r\nOverview\r\nuse of distinct()\r\n\r\nImporting survey information\r\nMissing values\r\n\r\nP6: Joining datasets\r\nCommentary on merges/join (left_join)\r\nJoin for our project\r\nConcatenating - Stacked joins\r\n\r\nP6: Saving the data files\r\nRDS approach\r\nExcel\r\nCSV/Table\r\n\r\n\r\nHelp!!!!!!!!\r\nUsing AI for coding\r\n\r\nHands-on component\r\nAdvanced topics [Advanced]\r\nQuality check packages\r\nvalidate package\r\npointblank package\r\n\r\nBig Data\r\nLearn functions\r\n\r\nResources\r\n\r\nDay’s objectives\r\nUnderstand workflow, why it is important, and how to do it using Rstudio/R\r\nHave an understanding of the tidyverse framework and its key packages\r\nProvide a bit of context/framework to working in R as many of us have learned piecemeal\r\nWork through an example\r\nKey packages\r\n\r\nWorkshop project: Wetland Data\r\n\r\n\r\n Download project data\r\n\r\n\r\nClick over to dataset webpage for overview of the wetland data\r\nWorkflows\r\nBasic of data analysis workflow\r\n\r\nExample of a R/Rstudio workflow\r\nSo those are the basics and suspect most will be rather familiar with that workflow. But, examples speak volumes. Let’s go through an example of the fundamentals of my R/Studio workflow:\r\nProject folder structure\r\n\r\nData storage: three main folders raw, rds, spatial\r\n\r\nR scripts : import/data cleaning, analysis\r\n\r\nstructure within the scripts (example of importData.r)\r\n\r\nReport writing: Rmarkdown file [day3]\r\nAdvantages of workflow\r\n“laziness” in data analysis programming\r\nreduce cognitive load\r\nreduce distractions (rabbitholes/“the weave”)\r\nefficiency\r\n\r\ncollaborations\r\na logical workflow makes for easy collaborations\r\n\r\nreproducibility\r\nlogical, clean workflows minimise error\r\nevery step is explicit in the code [no black boxes]\r\n\r\nCaveat on my R/Rstudio workflow\r\nMy R/Rstudio workflow has evolved over the years and reflects historical contingencies (e.g. working as analyst in pharmaceutical for 9 months, influential people I happened to meet).\r\nThis workshop will work off this framework but this workflow might not fit you per se. However, for those developing their workflow, it is useful to have an initial foundation in which to work off. So, as we go through this workshop, think about how you personalise the workflow for you:\r\nhow might you integrate your current process with the workflow presented?\r\nwhat unique aspects of your work need to be integrated into the workflow?\r\nwhat is the role of collaborators and meshing workflows?\r\nUpfront thinking about this is not time-wasted. If you have a great structure that inherently works for you, you will be more efficient.\r\nReady, set, go….\r\nOkay, we have the data and understand the project goals, design, and key questions. Let’s go through the basic steps to get going…\r\nP1: Setup your project folder\r\nCreate Rproject using Rstudio\r\n\r\nCreate folder structure\r\n\r\n\r\nCheatsheets had been made for many packages and are really useful. See Rstudio IDE to Positron https://github.com/posit-dev/positron/wiki for note about new IDE\r\nCreate an importData.r script\r\n\r\nLet’s talk a look at my standard import script\r\n\r\n\r\nClick the outline button in upper right of the script screen to show the outline of script. The outline is created by the #### Text ####\r\nTidyverse\r\nAt the top of my script, I put library(tidyverse)\r\n\r\n\r\nTidyverse is big topic but here are key points that I want you know:\r\nTidyverse is an opinionated approach to data analysis and has spurred a movement of tidy-everything (tidygraphs, tidybayes, sf package, and so much more)\r\nRather than a package per se, it is really a collection of packages (aka meta-package)\r\nTidyverse goals\r\nmake R code more consistent\r\nmake R code more readable for humans (e.g. uses words like filter, select, mutate, summarise)\r\ncreate a cult\r\n\r\nSide note - Version control/GitHub [Advanced topic]\r\nAs part of my due-diligence, I have to mention version control. In short, version control is external software that keeps track of all your changes (so you can go back if something breaks in your code) and very helpful with collaborations (e.g. allowing you both code away and then merging those changes).\r\nFor R/Rstudio, Git/GitHub is the most popular (and can be directly integrated). Now, Git is the version control software locally on your computer and it does the tracking of all changes In contrast, GitHub is an online platform in which you can upload those changes (and project folder/files) and is mainly useful for the collaboration/sharing (plus some other useful features)\r\nKey points on Git/GitHub:\r\nIf not collaborating, the overhead of Git (learning, initial setup, random breaks) might not worth it to you. You still have version histories via Onedrive to save you (not as easy to shift through as Git).\r\nIf collaborating, it really is the best approach that will save you effort in the long run.\r\nIt is worth playing around with GitHub online so you know how to navigate the website (this workshop will help with that). GitHub is a rich resource with example code and useful packages not on CRAN. GitHub project (aka repositories) can look intimidating at first.\r\nRstudio has git integration that makes it easier to work with, though the github desktop\r\nGitHub has GitHub pages which is hosting this website [workflow: 1) write RMD files in Rstudio; 2) upload to GitHub; 3) GitHub publishes at https:username.github.io/project_id]\r\nResearchers will host their packages on GitHub instead of CRAN\r\n\r\nCRAN is repository for packages and is teh default place to look for a package. As CRAN has certain rules/time-lags, some by-pass using other repositories like GitHub\r\nP2: Importing data\r\nAfter setting up environment, on to the import stage\r\nimport philosophy\r\nIn my workflow, I view data files as “locked(/read-only)”. This is part of the reproducibility pathway. So this means for me:\r\nI do not change filenames once the data go in that folder [e.g. if you do, you will forget where your file came and have to go digging]\r\nI will not touch anything in the files.\r\nEither I code around (so anyone can see the change)\r\nSend back to get it fix\r\n\r\nKey importing functions\r\nexcel files\r\nFor this, we will use readxl package\r\n\r\nThere are other read excel packages but this one just works\r\n\r\n\r\n  library(readxl)\r\n  read_xls()\r\n  read_xlsx()\r\n\r\n\r\nYou should be aware of how it handles missing data. In read_xlsx() you can specify an argument na= to indicate what cell values should be treated as missing.\r\n\r\n\r\n  library(readxl)\r\n  read_xlsx( 'data/raw/missing.xlsx', na='' ) # default\r\n\r\n# A tibble: 5 × 2\r\n    var missing\r\n  <dbl> <chr>  \r\n1     1 NA     \r\n2     2 na     \r\n3     3 .      \r\n4     4 <NA>   \r\n5     5 Na     \r\n\r\n  read_xlsx( 'data/raw/missing.xlsx', na=c('NA','na') )\r\n\r\n# A tibble: 5 × 2\r\n    var missing\r\n  <dbl> <chr>  \r\n1     1 <NA>   \r\n2     2 <NA>   \r\n3     3 .      \r\n4     4 <NA>   \r\n5     5 Na     \r\n\r\ncsv/txt\r\nFor reading csv/txt\r\n\r\n\r\n# from readr package in tidyverse\r\n  read_csv(file_name)    # comma delimited\r\n  read_table(file_name)  # tab delimited\r\n  read_delim(file_name, delim='#') # or other type of delimited \r\n  \r\n  ?read_csv  # to see arguments/options for importing\r\n\r\n\r\nDatabase [Advanced]\r\nSome possible packages to check:\r\nRMySQL: This package allows you to connect to MySQL databases and execute SQL queries1.\r\nRPostgreSQL: This package is designed for connecting to PostgreSQL databases1.\r\nRSQLite: This package is used for SQLite databases and is great for lightweight, serverless database applications1.\r\nRODBC: This package provides a way to connect to any database that supports ODBC (Open Database Connectivity)1.\r\nRJDBC: This package uses Java Database Connectivity (JDBC) to connect to a wide variety of databases1.\r\nImporting our project files\r\n\r\n\r\n#_____________________________________________________________________________  \r\n#### Section 1: site information ####\r\n# check sheet names in excel file\r\n  excel_sheets(file_excel)\r\n  \r\n  # bring in site data      \r\n  ds_site <- read_excel(file_excel, sheet = 'site_info')\r\n  \r\n  # take a look\r\n  head(ds_site)  # quick peak\r\n  View(ds_site)  # check out a datatable version...can also just click the object in the \r\n                 # environment panel in upper right\r\n  names(ds_site)  # have a look at object names\r\n\r\n\r\nP3: Data cleaning\r\nOverview of basics\r\nNow we have the first file in. That is the easy bit. However, we need to clean up dataset(s) so that they are ready for analysis. By cleaning I mean: - column management + renaming columns for coding + drop extra columns and extra rows + makes sure column types (e.g. numeric, character, factors, date) are correct + data entry errors in columns + string manipulations - check design (aka get intimate ❤️ with the data) + extract out the design elements to confirm it matches (or identity where is does not) + missing data - join together response variables, design, and covariates - restructure data for analysis (e.g. long vs wide form)\r\ncolumn management\r\ndropping/keeping columns - select()\r\nthe select() is used to select the columns you want. Here, we use it to drop the two columns we do not. The ‘…10’ was created automatically by read_excel() and not needed. We delete the columns using (-) negative in-front of the column we want to drop. We also learn if a column name has whitespace (or other special characters like %, #, …) you need to use quotes to refer to it.\r\n\r\n\r\n  # let's delete that the extra column... use select(data, -var)\r\n  ds_site <- select( ds_site, -...10)  # drop the ...10\r\n#  ds_site <- select( ds_site, -Drop me)  # this does not work\r\n  ds_site <- select( ds_site, -\"Drop me\")  # need to use either \"Drop me\" or `Drop me` if you have whitespace\r\n\r\n\r\nrenaming columns - rename()\r\nYou will want to rename columns. It is important to think about what makes a good column name for you. I stick to the following\r\nno whitespaces, no special characters, not too long, (for me) all lowercase\r\nprefixes or suffixes (useful for select functions) [e.g. dates are dt_, datetime is dttm_, site info is site_)\r\n\r\n\r\n  # now, let's rename [I will show two ways: rename() and within select() ]\r\n  ds_site <- ds_site %>% \r\n    rename( site_lat = Transect.Start.Latitude,\r\n            site_lon = Transect.Start.Longitude ) %>% \r\n    select(cma, wetland=wetland_name, transect_id, fence, grazing, starts_with('site_') )\r\n\r\n\r\ncheck column type and convert - mutate()\r\nWhen you want to create a new column or modify an existing column, mutate() is the key function. We used it to convert lat/lon from character to numeric.\r\n\r\n\r\n # now, let's check column type\r\n  head(ds_site)  # <chr> = character\r\n  \r\n  # convert lat/lon to numeric...need to use mutate and will use as.numeric()\r\n  ds_site <- mutate(ds_site, \r\n                    site_lon_n = as.numeric(site_lon),\r\n                    site_lat_n = as.numeric(site_lat) ) # note the warning\r\n  filter(ds_site, is.na(site_lon_n)|is.na(site_lat_n) )  # warning cause it was missing..will get later from researcher\r\n\r\n  # now, I want to keep the site_lat/site_lon column but as numeric \r\n  ds_site <- select(ds_site, -site_lon, -site_lat) %>% \r\n              rename( site_lat = site_lat_n, site_lon = site_lon_n )\r\n  head(ds_site) # looking good\r\n\r\n\r\n\r\n\r\n# commmon conversion functions\r\n  as.numeric()\r\n  as.character()\r\n  as.factor()\r\n  as_date()  # note there is as.Date() and as_date()...use the later\r\n  as_datetime()  # note there is as.Date() and as_date()...use the later\r\n\r\n\r\nFor dates, this is another whole topic. I highly recommend you use the lubridate package for all date handling. See date webpage for a bit more info\r\nFor factors, be very careful when converting…\r\n\r\n\r\n# factors are in essence an ordinal in that they are stored as numbers with labels\r\n  intensity <- c('low','medium','high') \r\n  intensity\r\n  class(intensity) # character\r\n  f_intensity <- as.factor(intensity)\r\n  f_intensity # now a factor and note the levels\r\n  as.numeric(f_intensity) # you get numbers\r\n\r\n# here is where the problem comes in...\r\n  ds_fish  <- data.frame( fish_id = c(1,2,6,7) ) # say you have fish id as a number\r\n  ds_fish  <- mutate(ds_fish, f_fish_id = factor(fish_id) ) # you convert fish_id to a factor for a model  \r\n  mutate(ds_fish, fish_id = as.numeric(f_fish_id) ) # you want to join it to another dataset and it needed to be numeric, you use as.numeric() you get the wrong numbers\r\n  # note - this use to be a bigger problem pre-tidyverse0\r\n\r\n\r\ndata issues and using filter()\r\nNext, we checked each column of interest to see if it matched our expectations. We introduced a string function str_to_sentence() to help. You also saw filter() and how to use that. The operators for fitler() are ==, !=, %in%. <aside>There are tons of string functions. Check the cheatsheet.\r\n\r\n\r\n\r\n # lets check each column to see if levels look correct..table useful here\r\n  table(ds_site$cma) # looks good\r\n  \r\n  table(ds_site$wetland) # looks good, no misspellings obvious, numbers not exactly 10\r\n  \r\n  table(ds_site$fence) # oops, yes vs Yes...capitalisation matters\r\n  ds_site <- mutate( ds_site, fence = str_to_sentence(fence) )  # let's use a string function\r\n  table(ds_site$fence) # Too easy!\r\n  \r\n  table(ds_site$grazing) # okay, was excepting just Crash/Press - was not expecting\r\n  ds_site <- filter(ds_site, grazing != 'NIL' )  # ! is used to say not\r\n  ds_site <- filter(ds_site, grazing %in% c('Crash','Press' ) )  # or you inverse way \r\n  table(ds_site$grazing) # all good\r\n\r\n\r\nP4: summarising data\r\nAfter you did initial column cleaning, we explored the experimental design to make sure that it was as expected. We found several issues and fixed them. We covered group_by() and combining with count(), summarise(), and mutate(). The key difference between summarise and mutate is that summarise returns a single row per group whereas mutate returns all initial row, adding in a the new variable.\r\nOnce grouped, the data stays that way until you ungroup() the data. This will trick you up!!!! If you do a calculation and cannot figure out why it is very wrong, check if data are still grouped. It is best to always ungroup() but that is not reality.\r\n\r\nIf you want to take the first(/last) number of rows, check out the family of slice() functions in dplyr. See the cheatsheet for examples\r\n\r\n\r\n##### let's check experimental design #####  \r\n  \r\n  # Now, I want to understand this transect_id.  Is it unique?\r\n  # option1 - if unique, then there will be one per wetland\r\n  ds_dups <- group_by(ds_site, wetland, transect_id) %>% count(name='n')  # count() \r\n  filter(ds_dups, n > 1)  # there are 8 were it is not\r\n  \r\n  # okay, not expecting that...let's add in the number of duplicates so can understand better\r\n  ds_site <- group_by(ds_site, wetland, transect_id) %>% \r\n              mutate( n_dups = n() )  # we use mutate here so it keeps all rows\r\n  filter(ds_site, n_dups==2)          # both No and Yes\r\n  filter(ds_site, n_dups==2) %>% distinct(wetland)  # in three wetlands...\r\n    # in the above code, I use distinct() to return the unique level for wetland\r\n    # but as you saw, it returned transect_id as well.  This is because dataset is still grouped\r\n  ds_site <- ungroup(ds_site)\r\n  filter(ds_site, n_dups==2) %>% distinct(wetland)  # in three wetlands...\r\n  \r\n  # I was told that this was a data entry error and I should drop the Fence==No ones \r\n  filter(ds_site, n_dups==2 & fence=='No')  # let's see the ones to drop..n=8\r\n  nrow(ds_site)\r\n  ds_site <- filter(ds_site, !(n_dups==2 & fence=='Yes') ) # \r\n  nrow(ds_site)  # 300-8 = 292...all good\r\n     \r\n # I want to know how many transects in each...let's use group_by() and summarise()\r\n  ds_sum <- ds_site %>% \r\n              group_by(wetland, fence) %>%  # break up into separate wetlands and do a calculation on each group\r\n              summarise( n_transects = n() )\r\n  filter(ds_sum, n_transects!=5)  # huh, thought there were only 5, check with researcher why\r\n\r\n\r\nFew other examples\r\n\r\n\r\n  mtcars\r\n  group_by(mtcars, cyl) %>% summarise( mean_mpg= round(mean(mpg),1), \r\n                                       sd_mpd= round(sd(mpg),1),\r\n                                       n = n() ) # n() is built in to count rows\r\n  # if you replace the same variable, all next calculation will be on the summarised variable\r\n  group_by(mtcars, cyl) %>% summarise( mpg= mean(mpg) %>% round(2), \r\n                                       max_mpg= max(mpg)%>% round(2),\r\n                                       min_mpg= min(mpg)%>% round(2) ) # see how min/max same\r\n\r\n\r\nP5: Restructure datasets\r\nOverview\r\nRestructuring is\r\n\r\n\r\n# I want to know the transect number differ between plot or are equal...restructure\r\n  ds_sum_wide <- pivot_wider( ds_sum, names_from=fence, values_from=n_transects )\r\n  head(ds_sum_wide)\r\n  filter(ds_sum_wide, No != Yes )  # might be worth double-checking if supposed to be\r\n\r\n\r\nuse of distinct()\r\nWe finished off Section1 by using the distinct() [so more]. distinct() is really useful for extracting out design. distinct() finds the unique combination of all the variables listed in the function. We used to get the unique combination of wetland, grazing, fence. You also saw n_distinct() which is useful in mutate to count the number of unique levels in variable.\r\n\r\n\r\n# let's check that the number wetland per cma\r\n  group_by(ds_site, cma) %>% summarise( n_wetland = n_distinct(wetland) ) # option 1\r\n  distinct(ds_site, cma, wetland) %>% group_by(cma) %>% count()  # options 2\r\n  \r\n  # let's check full design using some of the techniques above\r\n  ds_design <- distinct(ds_site, wetland, grazing, fence) %>% \r\n                group_by(wetland) %>% \r\n                  summarise(n_grazing=n_distinct(grazing), \r\n                         n_fence = n_distinct(fence) )\r\n  # any \r\n    filter( ds_design, n_grazing>1 ) # nothing\r\n    filter(ds_design, n_fence!=2) # nothing\r\n  \r\n# we have wetland, grazing, fence, transect_id but missing temporal component\r\n\r\n\r\nImporting survey information\r\nMissing values\r\nA new topic learned in Section 2 was NAs. We used the is.na() function to filter out this cells. NA are special characters and you cannot just do filter(ds_site, wetland==NA) or filter(ds_site, wetland=='NA'). The former statement caused R to look for a variable called ‘NA’ and the latter makes R look for variable level ‘NA’.\r\n\r\nNA can be tricky. In statistical models, any row with NA will be dropped quietly. When using functions like mean(), sd(), NAs will return NAs.\r\n\r\n\r\n#_____________________________________________________________________________  \r\n#### Section 2: import survey info ####\r\n  ds_survey <- read_csv('data/raw/df_survey_info.csv') \r\n  ds_survey\r\n  \r\n  ds_survey <- ds_survey %>% \r\n                  select(-...1) %>% \r\n                  rename( dt_survey = date ) # I prefer dt_xxx for my dates\r\n  # check columns\r\n  ds_survey  # all <chr> except for <date> as expected\r\n  summary(ds_survey) # missing dates for dt_survey\r\n  \r\n  # let's check out the NAs...using is.na() function\r\n  filter(ds_survey, is.na(dt_survey))  # ...check with researcher\r\n  ds_survey <- filter(ds_survey, !is.na(dt_survey) )  # let's say they should be deleted\r\n\r\n  # I would check if any transects not in the site dataset\r\n  filter(ds_survey, !(transect_id %in% ds_site$transect_id) )\r\n  filter(ds_site, wetland == 'Hoch') # yep no wetland that is Hoch...oh that is right, it is the NIL we dropped\r\n  \r\n  filter(ds_survey, str_detect(transect_id,'Hoch')) # we want to drop these\r\n  ds_survey <- filter(ds_survey, !str_detect(transect_id,'Hoch')) # drop them\r\n\r\n  # okay, I want to check out dates \r\n  ggplot(ds_survey, aes(dt_survey)) + geom_dotplot()  # showing a graph...matches our expectation (no odd date)\r\n\r\n  # survey dataset is looking good\r\n  #now let's join survey info and site info \r\n  # first, I want to create wetland in the survey dataset\r\n  # I noticed an NA in graph\r\n  ds_survey <- separate_wider_delim(ds_survey,\r\n                                    cols = transect_id,\r\n                                    names=c('wetland','transect_no'),\r\n                                    delim='_',\r\n                                    cols_remove=F)\r\n  ds_survey \r\n  ds_survey <- ds_survey %>% select(-transect_no) # not needed at this point\r\n  \r\n  # second, check that there are not duplicates for survey_id, dt_survey\r\n  group_by(ds_survey, survey_id, dt_survey) %>% count() %>% filter(n>1) # yippe, all clean\r\n\r\n\r\nP6: Joining datasets\r\nmerges/combining joins\r\n\r\n\r\n\r\n# whole suite joins....\r\n  left_join()  # stick this one\r\n  right_join() # just switch around to left_join\r\n  full_join()  # safer to create the fuller design (e.g. expand.grid) and then left_join() into it\r\n  inner_join() # safer to do left_join() and then filter()\r\n  outer_join() # just stick to left_join and filter()\r\n\r\n# note- there is bind_cols() but best to avoid and use a left_join() instead\r\n\r\n\r\nCommentary on merges/join (left_join)\r\nYou will join datasets are the time. In joins, (usually) you have two datasets and have something in common (e.g. site_id). For instance, you may have response variable in one dataset and covariates in another. You want to combine together.\r\nBe warned, joins can be major pitfalls!!!! If you are not intimate ❤️ with your datasets, it can lead to big issues. We showed this in our code in which we “forgot” to include transect_id:\r\n\r\n\r\n# as an example, left_join will warn you if it is not a one-to-one join\r\n  ds_mistake <- left_join( ds_survey, select(ds_site, wetland, fence, grazing) )\r\n  nrow(ds_mistake) # way bigger\r\n\r\n\r\nWhat happened here is called a one-to-many join. I was expecting the left dataset (the first dataset) to have the same number of rows before and after the join. Instead, it has more, creating fake data in essence (and increasing sample size, decreasing SEs, leading to erroneous stats). Therefore, you really need to keep tabs on number of rows.\r\nLuckily, dplyr::left_join() brought in a warning for this (I had my own function to warn me about this). My advice for you is to only use left_join() and make it a one-to-one join. Be careful being fancy in one-to-many and many-to-many (full_join) joins. Sometimes it is safer to work in separate steps rather than one big step.\r\nJoin for our project\r\nNow that is off my chest, let’s recall the joins we needed for our project…\r\n\r\n\r\n# now join in grazing, fence info...use a left_join and join by wetland, transect_id\r\n  nrow(ds_survey)\r\n  ds_survey <- left_join( ds_survey, \r\n                          select(ds_site, wetland, transect_id, fence, grazing) )\r\n  nrow(ds_survey) # same number as before, all good\r\n  \r\n  # as an example, left_join will warn you if it is not a one-to-one join\r\n  ds_mistake <- left_join( ds_survey, select(ds_site, wetland, fence, grazing) )\r\n  nrow(ds_mistake) # way bigger\r\n  \r\n#_____________________________________________________________________________  \r\n#### Section 3: import richness ####\r\n  excel_sheets(file_excel)\r\n  ds_richness <- read_excel(file_excel, sheet='spp_rich' )\r\n  ds_richness  # column types look good\r\n  \r\n  # only keeping native\r\n  ds_richness <- select(ds_richness, survey_id, native)\r\n  \r\n    \r\n  # let's add richness to ds_survey\r\n  # going to do a few checks\r\n  filter(ds_richness, !(survey_id %in% ds_survey$survey_id) ) # all good!\r\n  \r\n  # do a histogram/density plot\r\n  ggplot(ds_richness, aes(native)) + geom_histogram() + \r\n    geom_density(aes(y=after_stat(count) ),alpha=0.2, fill='red') # no obvious outliers at this point\r\n\r\n  # add richness to ds_survey \r\n  ds_richness_out <- left_join(ds_survey, \r\n                         ds_richness)\r\n  ds_richness_out  \r\n  \r\n  filter(ds_richness_out, is.na(native)) # 46 rows of NA...check with researcher\r\n\r\n\r\nConcatenating - Stacked joins\r\nI did not use concatenating in this dataset. A common use for this is when you have multiple files produced that have the same data structure (e.g. acoustic data from loggers, camera trap data, acoustic moth data) and you need to stack them together for the analysis. You will use bind_rows() for that.\r\n\r\n\r\n  ds1 <- data.frame( plot='plot1', quadrat=1:2, species_richness=c(10,20), comments=c('none','something'))\r\n  ds1\r\n\r\n   plot quadrat species_richness  comments\r\n1 plot1       1               10      none\r\n2 plot1       2               20 something\r\n\r\n  ds2 <- data.frame( plot='plot2', quadrat=1:2, species_richness=c(2,4))\r\n  ds2\r\n\r\n   plot quadrat species_richness\r\n1 plot2       1                2\r\n2 plot2       2                4\r\n\r\n  bind_rows(ds1, ds2)  \r\n\r\n   plot quadrat species_richness  comments\r\n1 plot1       1               10      none\r\n2 plot1       2               20 something\r\n3 plot2       1                2      <NA>\r\n4 plot2       2                4      <NA>\r\n\r\n\r\nthere is a bind_cols() to add columns next to each other. I rarely use, preferring to use a join to make sure everything lines up\r\nP6: Saving the data files\r\nFinally, we have sparkly-clean datasets (and hopefully no hidden dirt under the rugs). The final step in the importData.r is to save in the data/rds to be used for the next step: modelling!\r\nRDS approach\r\nAt the end of the import and clean stage, I save the dataset(s) as an RDS file. The advantage of this method is that you are saving the dataset(s) in native R format so when you bring back in, there is no conversion.\r\n\r\n\r\n#_____________________________________________________________________________  \r\n#### Section 5: save datasets ####\r\n    \r\n  saveRDS(ds_richness_out, 'data/rds/ards_richness.rds')    \r\n  saveRDS(ds_ht_out, 'data/rds/ards_height.rds')    \r\n\r\n\r\nExcel\r\nYou may want to save an excel file to share. You will need the writexl package for this.\r\n\r\n\r\n# excel file\r\n  writexl::write_xlsx( ds_richness, 'data/export/ards_richness.xlsx') \r\n\r\n\r\nCSV/Table\r\nOr if you prefer low overhead files (smaller size).\r\n\r\n\r\n readr::write_csv( ds_richness,file='ards_richness.csv')\r\n readr::write_delim( ds_richness,file='ards_richness.txt',delim = '\\t') # tab\r\n\r\n\r\nHelp!!!!!!!!\r\nWhen learning and using R, you will get stuck and need help. There are variety of help sources we use on a daily basis.\r\nBuilt-in to Rstudio are help files. This provides an initial starting place if you are interested in how to use a specific function. You just need to put “?” before the function and press run: ?mean() or use the Help tab\r\nGoogle search (e.g. stackoverflow, blogs) can be really helpful for finding new functions, packages, etc.\r\nBiometrics - happy to provide guidance/tips/thoughts\r\nARI’s QEARI - hacky hour or Teams chat\r\n\r\nAI is becoming more and more useful [I am finding the most useful]\r\nUsing AI for coding\r\nA few AI I use regularly:\r\nCopilot (Microsoft): allrounder AI which we have a free subscription at ARI. It does a decent job [I use this one daily]\r\nChatGPT: another allrounder similar to Copilot\r\nClaude: prefer this one for writing text but seems to work well too\r\nCopilot-GitHub: paid subscription but supposedly really good for programming\r\nFor those not regularly using AI, there are a few things that can help when running your searches for help:\r\nwhen asking for an example, include lots of specific to get closer to what you want. You may want to tell it to use only tidyverse functions for the example code. I often ask it to use a built-in dataset for the example so that I can check it right away.\r\nExperiment and Iterate: Don’t be afraid to experiment with the code generated by AI. Modify it, run it, and see what happens. If something is not working, you can ask point out the error and see if it can fix it\r\nUse AI for Debugging: If you encounter errors, AI tools can help you debug your code. They can suggest fixes and optimizations, making the debugging process less daunting.\r\nGive it try yourself copilot - type…“in R and using mtcars dataset show me how to get the mean mpg by each cyl level?” - if it used the aggregate() function, type the followup: “can you use tidyverse functions instead?”\r\n\r\nHands-on component\r\n\r\n\r\n Download project data\r\n\r\n\r\n\r\n\r\nYour Task: Create your Rproject for the workshop and import data for analysis-ready datasets\r\nDownload the workshop data (refer dataset webpage for background info if needed)\r\nSetup your workflow\r\nthink about your naming convention for the project names (does it have year, projectID, client). [My standard is “{Year}_{Researcher}_{Topic}”]\r\ncreate Rproject\r\ncreate folder structure (how do you want yours to look?)\r\n\r\nadd in datasets to your data folder\r\ncreate your import script file\r\nwhat is your naming convention for R script (e.g. importData.R, 01_import.R, bringInData.R)\r\ncreate your R script structure (e.g. do you have headers, bookmarks)\r\n\r\nfill in script for what we have done so far\r\ngo through this “lecture” and grab script and paste into your script\r\ntest it along the way - paste in, highlight and run\r\n\r\nfinally, write your own script to bring maximum plant height tab\r\nimport plant height data from excel file\r\nclean the data\r\nmerge into the species richness dataset\r\nsave as a plant height RDS file (and try writexl)\r\n\r\nCheck compare your version to my using link below\r\n\r\n\r\nBelow is the link to my Rproject version:\r\n\r\n\r\n Download Rstudio project\r\n\r\n\r\n\r\n\r\nPermutation for Newbies\r\nIf your are new to R, you may have found the material a bit overwhelming. If so, it may be best to just download the Rproject that I have created for today and work your way through that.\r\nSteps:\r\nDownload the Rproject folder (see button above)\r\nSave in your preferred location on your laptop\r\nOpen the project by double clicking using the file with the .Rproj extention\r\n\r\nOnce in Rstudio, open the r/import.R file\r\nRun through the script highlighting each section and clicking Run button (or Ctrl+Enter shortcut)\r\n\r\nAdvanced topics [Advanced]\r\nQuality check packages\r\nIf you know that you will be updating the import of files repeatedly, it can be worthwhile to do some upfront QC/QA checks to ensure that the data coming in is clean. There are some packages to help with that.\r\nA few things to consider with these QA/QC:\r\nAre your data coming a database that already implements QA/QC [ideal world]\r\nAre you working with large datasets (lots of columns, separate tables)\r\nIs this an ongoing project so you will be updating regularly the analyses\r\nIs there an automation process [press run and get outputs at the end]\r\nIf you working with clean data or once off datasets, it may be best to just write your data cleaning checks as we have done here. The overhead of these packages unlikely to outweigh the added benefits. Once you are automating/updating dataset, the overhead is often easily outweighed.\r\nHere are two packages to have a look at…\r\nvalidate package\r\nYou can find an overview of this package on its github page\r\nThe validate package is intended to make checking your data easy, maintainable, and reproducible.\r\npointblank package\r\nYou can find an overview of this package on its github page\r\nThis package takes an data.frame, runs through a bunch of checks on each column and provides an useful report highlighting errors and warning (might be an error).\r\nThe basic steps are below in their graphic:\r\n\r\nMy thoughts: This package seems like a step-up in level of QA/QC from validate. I have used a few times but have not regularly implemented due to overhead. Probably becuase my brain is full, it takes a bit of re-leaning each time and I have not taken the time to create a template that minimises that effort.\r\nBig Data\r\nWhen data starts to get big (millions of row), tidyverse can struggle with speed. This is huge topic but will direct you to a couple useful packages that allows you to just learn dplyr but use other quicker processes\r\ndtplyr package\r\nunder the hood it uses the data.table package\r\nbut you write code in tidy/dplyr\r\n\r\ndbplyr package [if working with a database]\r\nhas the database do all the processes and return the result back to R\r\ntakes advantage of cloud computing\r\n\r\nLearn functions\r\nA fundamental skill in coding is creating functions. Packages are basically just collection of functions. I write functions all the time. This helps keep code clean, are reusable (so saves time), helps prevent cut/paste errors, and improves readability/reproducibility. Definitely worth learning as some point.\r\n\r\n\r\nmy_plus_function <- function(x,y){\r\n    x + y\r\n}\r\n\r\nmy_plus_function(1,2)\r\n\r\n\r\nResources\r\nObviously, we have only scratched the surface of the topics here. Here are couple books I have used but lots out there.\r\nR for Data Science\r\nAdvanced R\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-09-15T10:30:02+10:00",
    "input_file": "day1-getting-data-into-shape.knit.md"
  },
  {
    "path": "sessions/day2-analysis-workflow/",
    "title": "Day 2: Analysis workflow",
    "description": "Statistical analysis: 1) statistical model; 2) extracting outputs, 3) displaying(/graphing)",
    "author": [
      {
        "name": "Paul Moloney",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-03",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nHeader1\r\n\r\nHeader1\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-09-04T11:58:26+10:00",
    "input_file": {}
  },
  {
    "path": "sessions/day3-report-writing-with-quarto/",
    "title": "Day 3: Report writing with Quarto",
    "description": "Learn about report making in R",
    "author": [
      {
        "name": "Paul Moloney",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-02",
    "categories": [],
    "contents": "\r\nHeader 1\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\nHeader 2\r\nHeader 3\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-09-04T10:07:36+10:00",
    "input_file": {}
  },
  {
    "path": "sessions/day4-shiny-apps/",
    "title": "Day 4: Shiny apps",
    "description": "General introduction",
    "author": [
      {
        "name": "Ben Fanson",
        "url": "https://bfanson.github.io/2024DADAworkshop/"
      }
    ],
    "date": "2024-09-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nDay’s learning objectives\r\n\r\nDay’s learning objectives\r\nHave a good understanding of what Shiny apps can do and how it might be useful for you\r\nUnderstand the core structure of Shiny apps: UI, Server, Reactivity\r\nLearn how to create and run an App locally as well as deploy on ARI’s shiny account\r\nFeel confident enough to grab example code from Rshiny gallery 🤞\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-09-12T07:23:32+10:00",
    "input_file": {}
  }
]
